{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"\u6b22\u8fce\u8d56\u5230\u6211\u7684\u5b66\u4e60\u7b14\u8bb0","text":"<p>\u8fd9\u4e2a\u7f51\u7ad9\u662f\u6211\u7684\u5b66\u4e60\u7b14\u8bb0\u4e0e\u5fc3\u5f97</p>"},{"location":"#_2","title":"\u5185\u5bb9\u5bfc\u822a","text":"<ul> <li>Langchain\u5b66\u4e60\u4ee3\u7801\u7b14\u8bb0</li> <li>\u5927\u6a21\u578b\u5fae\u8c03</li> <li>\u6570\u636e\u95ee\u7b54\u754c\u9762</li> </ul>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/","title":"Lora\u5fae\u8c03","text":""},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#1lora","title":"1.\u4ec0\u4e48\u662fLora","text":""},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#11","title":"1.1\u5168\u91cf\u5fae\u8c03\uff1a","text":"<p>\u987e\u540d\u601d\u4e49\uff0c\u5c31\u662f\u628a\u5927\u6a21\u578b\u6240\u6709\u7684\u53c2\u6570\u90fd\u8fdb\u884c\u5fae\u8c03\u3002\u4f46\u662f\u968f\u7740\u5927\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u52a8\u8f84\u5927\u51e0\u5341\u51e0\u767eB\uff0c\u5168\u91cf\u5fae\u8c03\u7684\u538b\u529b\u4e5f\u8d8a\u53d1\u7684\u5927\u3002  </p>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#12","title":"1.2\u66ff\u4ee3\u65b9\u6848\uff1a","text":"<ul> <li>Adapt Tuning\uff1a\u5728\u8bad\u7ec3\u4e2d\u52a0\u5165Adapt\u5c42\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u56fa\u5b9a\u5176\u4ed6\u53c2\u6570\u53ea\u66f4\u65b0Adapt\u5c42\u7684\u53c2\u6570</li> <li>P-tuning\uff1a\u4ed6\u5728\u6bcf\u4e2a\u95ee\u9898\u4e4b\u524d\u6dfb\u52a0\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u524d\u7f00\uff0c\u5c31\u53ef\u4ee5\u7528\u6765\u4e13\u95e8\u8bad\u7ec3\u4e00\u4e2a\u4e13\u4e1a\u6280\u80fd\u3002\u5982\u60f3\u5fae\u8c03\u5927\u6a21\u578b\u6765\u4e13\u95e8\u56de\u7b54\u5386\u53f2\u95ee\u9898\uff0c\u800c\u4e0d\u5e0c\u671b\u4fee\u6539\u6a21\u578b\u7684\u6240\u6709\u53c2\u6570\u3002\u5c31\u53ef\u4ee5\u4f7f\u7528P-tuning\uff0c\u5728\u6bcf\u4e2a\u95ee\u9898\u4e4b\u524d\u6dfb\u52a0\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u524d\u7f00\uff0c\u5982 \u201c\u5386\u53f2\u4e13\u5bb6\uff1a\u201d  </li> </ul>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#13lora","title":"1.3Lora\u5fae\u8c03\uff1a","text":"<p>LoRA \u7684\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u8bad\u7ec3\u4f4e\u79e9\u77e9\u9635\u6355\u6349\u4efb\u52a1\u7279\u5b9a\u7684\u53d8\u5316\uff0c\u800c\u4fdd\u6301\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5927\u90e8\u5206\u53c2\u6570\u4e0d\u53d8\u3002\u4f4e\u79e9\u77e9\u9635\u8868\u793a\u4fe1\u606f\u7684\u538b\u7f29\uff0c\u53ea\u9700\u5c11\u91cf\u53c2\u6570\u5c31\u80fd\u8fd1\u4f3c\u63cf\u8ff0\u6570\u636e\u7684\u4e3b\u8981\u7279\u5f81\u3002 \u5047\u8bbe\u4f60\u6709\u4e00\u5e45\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\u7247\uff08\u9884\u8bad\u7ec3\u6a21\u578b\uff09\uff0c\u4f46\u53ea\u60f3\u8c03\u6574\u989c\u8272\uff08\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\uff09\u3002\u4f60\u4e0d\u9700\u8981\u4fee\u6539\u6bcf\u4e2a\u50cf\u7d20\uff08\u5168\u53c2\u6570\u5fae\u8c03\uff09\uff0c\u800c\u662f\u53ef\u4ee5\u5e94\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u6ee4\u955c\uff08\u4f4e\u79e9\u77e9\u9635\uff09\u6765\u8fbe\u5230\u6548\u679c\u3002\u8fd9\u79cd\u6ee4\u955c\u53ef\u4ee5\u7528\u5f88\u5c11\u7684\u53c2\u6570\u63cf\u8ff0\uff0c\u5374\u80fd\u663e\u8457\u6539\u53d8\u56fe\u7247\u7684\u6574\u4f53\u8272\u8c03\u3002\u8fd9\u5c31\u7c7b\u4f3c\u4e8e LoRA \u7684\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\u3002</p>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#2lorabliblipeft","title":"2.Lora\u5fae\u8c03blibli\u5927\u6a21\u578b(PEFT)\uff1a","text":""},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#21","title":"2.1\u6a21\u578b\u4e0b\u8f7d\uff1a","text":"<pre><code>import torch\nfrom modelscope import snapshot_download, AutoModel, AutoTokenizer\nimport os\n\nmodel_dir = snapshot_download('IndexTeam/Index-1.9B-Chat', cache_dir='model_path', revision='master') #\u7248\u672c\u9009master\u662f\u56e0\u4e3a\u4e00\u822cmaster\u662f\u7a33\u5b9a\u7684\u7248\u672c\n</code></pre>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#22","title":"2.2\u6307\u4ee4\u96c6\u7684\u6784\u5efa","text":"<p>\u8bad\u7ec3\u96c6\u9009\u62e9\u4ece\u7f51\u4e0a\u627e\u5230\u4e00\u4e2a\u5173\u4e8e\u7504\u5b1b\u8bed\u6c14\u7684\u8bad\u7ec3\u96c6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a  </p> <p></p> <p>\u5176\u4e2d\uff0cinstruction \u662f\u7528\u6237\u6307\u4ee4\uff0c\u544a\u77e5\u6a21\u578b\u5176\u9700\u8981\u5b8c\u6210\u7684\u4efb\u52a1\uff1binput \u662f\u7528\u6237\u8f93\u5165\uff0c\u662f\u5b8c\u6210\u7528\u6237\u6307\u4ee4\u6240\u5fc5\u987b\u7684\u8f93\u5165\u5185\u5bb9\uff1boutput \u662f\u6a21\u578b\u5e94\u8be5\u7ed9\u51fa\u7684\u8f93\u51fa\u3002  </p> <p>\u6570\u636e\u683c\u5f0f\u5316\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>def process_func(example):\n    MAX_LENGTH = 384    # \u5206\u8bcd\u5668\u4f1a\u5c06\u4e00\u4e2a\u4e2d\u6587\u5b57\u5207\u5206\u4e3a\u591a\u4e2atoken\uff0c\u56e0\u6b64\u9700\u8981\u653e\u5f00\u4e00\u4e9b\u6700\u5927\u957f\u5ea6\uff0c\u4fdd\u8bc1\u6570\u636e\u7684\u5b8c\u6574\u6027\n    input_ids, attention_mask, labels = [], [], []\n    instruction = tokenizer(f\"&lt;unk&gt;system\u73b0\u5728\u4f60\u8981\u626e\u6f14\u7687\u5e1d\u8eab\u8fb9\u7684\u5973\u4eba--\u7504\u5b1breserved_0user{example['instruction'] + example['input']}reserved_1assistant\", add_special_tokens=False)  # add_special_tokens \u4e0d\u5728\u5f00\u5934\u52a0 special_tokens\n    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # \u56e0\u4e3aeos token\u4e5f\u662f\u8981\u5173\u6ce8\u7684\u6240\u4ee5 \u8865\u5145\u4e3a1\n    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n    if len(input_ids) &gt; MAX_LENGTH:  # \u505a\u4e00\u4e2a\u622a\u65ad\n        input_ids = input_ids[:MAX_LENGTH]\n        attention_mask = attention_mask[:MAX_LENGTH]\n        labels = labels[:MAX_LENGTH]\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n</code></pre> <p>\u8fd9\u4e2a\u8bad\u7ec3\u96c6\u7684\u6784\u5efa\u4e5f\u6709\u70b9\u50cfP\u2014tuning\uff0c\u5728\u8bad\u7ec3\u4e2d\u52a0\u4e86\u524d\u7f00\u201c\u73b0\u5728\u4f60\u8981\u626e\u6f14\u7687\u5e1d\u8eab\u8fb9\u7684\u5973\u4eba--\u7504\u5b1b\u201d process_func\u51fd\u6570\u8fd4\u56de\u4e00\u4e2a\u5b57\u5178\u5305\u542b input_ids\u3001attention_mask \u548c labels\u4e09\u4e2a\u90e8\u5206\uff1a * input_ids\uff1a\u5c06\u8f93\u5165\u7684\u6587\u672c\u7f16\u7801\u4f20\u7ed9\u6a21\u578b * attention_mask\uff1a\u8868\u793a\u6a21\u578b\u9700\u8981\u5173\u6ce8\u54ea\u4e9b\u5730\u65b9\uff0c\u4e00\u822c\u586b\u5145\u7684\u4e0d\u592a\u9700\u8981\u5173\u6ce8 * \u56de\u7b54\u7684\u6587\u672c\u7f16\u7801</p>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#23tokenizer","title":"2.3\u52a0\u8f7d\u6a21\u578b\u548cTokenizer","text":"<pre><code>tokenizer = AutoTokenizer.from_pretrained('/root/autodl-tmp/Tsumugii24/Index-1.9B-Chat/', use_fast=False, trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained('/root/autodl-tmp/Tsumugii24/Index-1.9B-Chat/', device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n</code></pre>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#24loraconfig","title":"2.4\u5b9a\u4e49LoraConfig:","text":"<pre><code>config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM, \n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    inference_mode=False, # \u8bad\u7ec3\u6a21\u5f0f\n    r=8, # \u51b3\u5b9a\u4e86\u4f4e\u79e9\u77e9\u9635\u7684\u7ef4\u5ea6\n    lora_alpha=32, # \u7f29\u653e\u56e0\u5b50\n    lora_dropout=0.1 # Dropout \u6bd4\u4f8b\n)\n</code></pre>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#25train","title":"2.5\u5b9a\u4e49Train\u7684\u53c2\u6570\u5e76\u8bad\u7ec3","text":"<pre><code>args = TrainingArguments(\n    output_dir=\"./output/Index-1.9B-Chat-lora\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    logging_steps=10,\n    num_train_epochs=3,\n    save_steps=100,\n    learning_rate=1e-4,\n    save_on_each_node=True,\n    gradient_checkpointing=True\n   )\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_id,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n)\n\ntrainer.train()\n\n</code></pre> <p>\u540e\u7b49\u5f85\u8bad\u7ec3\u5c31\u597d\u4e86...  </p>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#26lora","title":"2.6\u52a0\u8f7dlora\u5fae\u8c03","text":"<p>\u5fae\u8c03\u5b8c\u6210\u540e\u5f53\u7136\u5c31\u5f97\u8c03\u7528\u5566:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nfrom peft import PeftModel\n\nmodel_path = 'model_path'\nlora_path = 'lora_path'\n\n# \u52a0\u8f7dtokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# \u52a0\u8f7d\u6a21\u578b\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\",torch_dtype=torch.bfloat16)\n\n# \u52a0\u8f7dlora\u6743\u91cd\nmodel = PeftModel.from_pretrained(model, model_id=lora_path, config=config)\n\nprompt = \"\u4f60\u662f\u8c01\uff1f\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"\u73b0\u5728\u4f60\u8981\u626e\u6f14\u7687\u5e1d\u8eab\u8fb9\u7684\u5973\u4eba--\u7504\u5b1b\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\n\ntext = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to('cuda')\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\nprint(response)\n</code></pre> <p>\u8fd9\u5c31\u5b8c\u6210\u4e86\u57fa\u7840\u7684\u5fae\u8c03\u4e86\uff0c1.9B\u7684\u6a21\u578b\u8bad\u7ec3\u96c6\u4e5f\u4e0d\u5927\uff0c\u6211\u57283090\u4e0a\u8bad\u7ec3\u4e0d\u5230\u534a\u4e2a\u5c0f\u65f6\u5c31\u5b8c\u6210\u4e86\u4e00\u6b21\u5fae\u8c03\u3002 \u540e\u7eed\u6211\u4e60\u60ef\u662f\u4f7f\u7528llama.cpp\u5c06\u6a21\u578b\u91cf\u5316\u4f20\u5230ollama\u8fdb\u884c\u8c03\u7528</p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch01_%E6%8F%90%E7%A4%BA%E8%AF%8D/","title":"ch01\u63d0\u793a\u8bcd","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch01_%E6%8F%90%E7%A4%BA%E8%AF%8D/#1","title":"1.\u6a21\u578b\u76f4\u63a5\u56de\u7b54","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch01_%E6%8F%90%E7%A4%BA%E8%AF%8D/#_1","title":"\u5bfc\u5165\u76f8\u5173\u6a21\u5757","text":"<pre><code>import os\nfrom langchain_openai import OpenAI\nfrom langchain_community.llms import HuggingFaceHub\nimport os\nos.environ['OPENAI_API_KEY'] = 'sk-'\n</code></pre>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch01_%E6%8F%90%E7%A4%BA%E8%AF%8D/#_2","title":"\u5bfc\u5165\u6a21\u578b\u8fdb\u884c\u56de\u7b54","text":"<pre><code>llm = OpenAI(\n             temperature=0.9,\n             max_tokens = 256)#temperature\u8d8a\u5c0f\u56de\u7b54\u8d8a\u4e25\u8c28\nprompt = f\"\u4f60\u7684\u7238\u7238\u662f\u8c01\u201c\nresponse = llm(prompt)\n</code></pre>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch01_%E6%8F%90%E7%A4%BA%E8%AF%8D/#2prompt","title":"2.\u7528prompt\u6a21\u5757\u56de\u7b54","text":"<p>\u4f18\u70b9\uff1a\u66f4\u5177\u4e13\u7528\u6027\uff0c\u66f4\u9002\u7528\u4e8e\u4e00\u4e2a\u4e13\u7528\u7684\u7cfb\u7edf</p> <pre><code>restaurant_template = \"\"\"\n\u4f60\u662f\u4e00\u4e2a\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u8bf7\u4f60\u5c06\u4ee5\u4e0b\u6570\u636e{data}\u6309\u503c\u4ece\u5927\u5230\u5c0f\u6392\u5217\n\"\"\"\n\nprompt = PromptTemplate(\n    input_variables=[\"restaurant_desription\"],\n    template=restaurant_template,\n)\n</code></pre>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch02_memery/","title":"ch02\u8bb0\u5fc6","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch02_memery/#1conversationbuffermemory","title":"1.ConversationBufferMemory:","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch02_memery/#_1","title":"\u5bfc\u5165\u76f8\u5173\u6a21\u5757\uff1a","text":"<pre><code>    from langchain.chains.conversation.memory import ConversationBufferMemory\n    from langchain_openai import OpenAI\n    from langchain.chains import ConversationChain\n</code></pre>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch02_memery/#_2","title":"\u8fdb\u884c\u95ee\u7b54\uff1a","text":"<pre><code>    llm = OpenAI( \n             temperature=0, \n             max_tokens = 256)\n     memory = ConversationBufferMemory()\n     conversation = ConversationChain(\n                  llm=llm, \n                  verbose=True, \n                   memory=memory)\n     conversation.predict(input=\"\u4f60\u597d\")\n</code></pre> <p>ConversationBufferMemory\u4f1a\u5c06\u7528\u6237\u4e0eai\u7684\u95ee\u7b54\u548c\u4e0b\u4e00\u6b21\u7684propmt\u4e00\u8d77\u4f20\u7ed9\u5927\u6a21\u578b\u8fdb\u884c\u95ee\u7b54\uff0c\u8fd9\u6837\u7684\u7f3a\u70b9\u5c31\u662f\u5f88\u5feb\u4f1a\u8d85\u51fatoken </p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch02_memery/#2conversationsummarymemory","title":"2.ConversationSummaryMemory","text":"<pre><code>    # memery\u66ff\u6362\u6210ConversationSummaryMemory\n    memory = ConversationSummaryMemory()\n    conversation = ConversationChain(\n                  llm=llm, \n                  verbose=True, \n                   memory=memory)\n     conversation.predict(input=\"\u4f60\u597d\")\n</code></pre> <p>*\u987e\u540d\u601d\u4e49\uff0cConversationSummaryMemory\u4f1a\u5c06\u7528\u6237\u4e0eai\u7684\u5bf9\u8bdd\u8fdb\u884c\u603b\u7ed3\u8fdb\u884c\u8bb0\u5fc6\u518d\u4f20\u7ed9\u5927\u6a21\u578b</p> <ul> <li>\u63d0\u793a\uff1a\u5728langchain0.1.17\u7248\u672c\u4ee5\u540e\u4f7f\u7528langchain.chains\u4f1a\u62a5\u8b66\u544alangchain.chains\u5df2\u7ecf\u4e0d\u518d\u4f7f\u7528</li> </ul>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch03_langchain%E4%BD%BF%E7%94%A8huggingface/","title":"ch03Huggingface","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch03_langchain%E4%BD%BF%E7%94%A8huggingface/#_1","title":"\u901a\u8fc7\u5728\u7ebf\u548c\u672c\u5730\u8c03\u7528\u4e24\u79cd\u65b9\u5f0f\u5206\u522b\u6765\u5b9e\u73b0","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch03_langchain%E4%BD%BF%E7%94%A8huggingface/#1googleflan-t5-xl","title":"1.\u901a\u8fc7\u5728\u7ebf\u7684\u65b9\u5f0f\u8c03\u7528google/flan-t5-xl\u8fdb\u884c\u95ee\u7b54\uff08\u7ed3\u5408\u63d0\u793a\u8bcd\uff09","text":"<pre><code>from langchain_core.prompts import PromptTemplate\nfrom langchain_community.llms import HuggingFaceHub\nfrom langchain.chains import LLMChain\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\n#\u5b9a\u4e49\u94fe\nllm_chain = LLMChain(prompt=prompt, \n                     llm=HuggingFaceHub(repo_id=\"google/flan-t5-xl\", \n                                        model_kwargs={\"temperature\":0, \n                                                      \"max_length\":64}))\nquestion = \"\u5c0f\u7c73\u96c6\u56e2\u7684CEO\u662f\u8c01?\"\n\nprint(llm_chain.invoke(question))\n</code></pre> <p>\u901a\u8fc7\u5728\u7ebf\u8c03\u7528\u7684\u65b9\u5f0f\u53ef\u80fd\u4f1a\u51fa\u73b0\u7f51\u7edc\u548ctoken\u7b49\u5404\u79cd\u83ab\u540d\u5176\u5999\u7684\u9519\u8bef\uff0c\u6240\u4ee5\u4e00\u822c\u6211\u4e0d\u600e\u4e48\u4f7f\u7528huggingface\u5728\u7ebf\u8c03\u7528\u7684\u65b9\u5f0f</p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch03_langchain%E4%BD%BF%E7%94%A8huggingface/#2lacal-model","title":"2.\u4f7f\u7528lacal model","text":"<pre><code>from langchain_community.llms import HuggingFacePipeline\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n\nmodel_id = 'google/flan-t5-large'# go for a smaller model if you dont have the VRAM\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True)#\u4f7f\u75288bit\u91cf\u5316\n\npipe = pipeline(\n    \"text2text-generation\",\n    model=model, \n    tokenizer=tokenizer, \n    max_length=100\n)\n\nlocal_llm = HuggingFacePipeline(pipeline=pipe)\n\nllm_chain = LLMChain(prompt=prompt, \n                     llm=local_llm\n                     )\n\nquestion = \"\u8c01\u662f\u5c0f\u7c73\u96c6\u56e2\u7684CEO?\"\n\nprint(llm_chain.invoke(question))\n</code></pre> <p>\u5f53\u7136\u4f7f\u7528Hugingfacehub\u4e5f\u4e00\u6837\u5f97\u6709Huggingface\u7684key\uff0c\u6211\u901a\u5e38\u559c\u6b22\u7528os\u5c06key\u4fdd\u5b58\u5728\u73af\u5883\u4e2d</p>"},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/","title":"\u6570\u636e\u95ee\u7b54","text":""},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#pandasaistreamlit","title":"\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8epandasai\u548cstreamlit\u7684\u6570\u636e\u95ee\u7b54\u7cfb\u7edf","text":""},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#csvexcel","title":"\u7b80\u4ecb\uff1a\u7528\u6237\u53ef\u4ee5\u4f20\u5165csv\u6216excel\u8868\u683c\u6587\u4ef6\uff0c\u53ef\u4ee5\u5bf9\u6570\u636e\u8fdb\u884c\u63d0\u95ee\u3002\u754c\u9762\u5982\u4e0b:","text":"<p>\u8bbe\u8ba1\u7528\u6237\u5171\u6709\u56db\u4e2a\u6a21\u578b\u53ef\u4ee5\u9009\u62e9\uff1a * \u706b\u5c71\u5f15\u64ce * Openai\uff08\u6709\u9700\u8981\u4ee3\u7406\u95ee\u9898\uff09 * ollama(\u5728\u672c\u5730\u6709\u8fd0\u884c\u6162\u7684\u95ee\u9898) * Groq\uff08\u540c\u6837\u6709\u4ee3\u7406\u95ee\u9898\uff09</p>"},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#_1","title":"\u6838\u5fc3\u4ee3\u7801\u89e3\u8bfb:","text":""},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#1","title":"1.\u5bfc\u5165\u6838\u5fc3\u6a21\u5757","text":"<pre><code>import streamlit as st \nimport pandas as pd\nfrom pandasai.llm import OpenAI\nfrom langchain_groq.chat_models import ChatGroq\nfrom langchain_community.llms import Ollama\nfrom pandasai import SmartDataframe, clear_cache\nfrom pandasai.connectors import MySQLConnector\nfrom pandasai.responses.streamlit_response import StreamlitResponse\nfrom io import StringIO, BytesIO\nimport sys\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom huoshan import VolcanoLLM\n\nplt.rcParams['font.sans-serif'] = ['SimHei']  # \u7528\u6765\u6b63\u5e38\u663e\u793a\u4e2d\u6587\u6807\u7b7e\nplt.rcParams['axes.unicode_minus'] = False    # \u7528\u6765\u6b63\u5e38\u663e\u793a\u8d1f\u53f7\n</code></pre>"},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#2","title":"2.\u8bbe\u7f6e\u6807\u9898\uff0c\u548c\u4f20\u5165\u6570\u636e\u6a21\u5757(\u6570\u636e\u53ef\u901a\u8fc7\u4e0a\u4f20\u548c\u6570\u636e\u5e93\u7684\u65b9\u5f0f\u4f20\u5165)","text":"<pre><code>st.set_page_config(page_title=\"Talk to Your Data\")\nst.title(\"Talk to Your Data \ud83d\udc3c\")\n\n# \u4e0a\u4f20 CSV \u6216 Excel \u6587\u4ef6\nuploaded_file = st.sidebar.file_uploader(\"Upload your CSV or Excel file\", type=[\"csv\", \"xlsx\"])\ndata = None\n    if uploaded_file is not None:\n        if uploaded_file.name.endswith(\".csv\"):\n            data = pd.read_csv(uploaded_file)\n        elif uploaded_file.name.endswith(\".xlsx\"):\n            data = pd.read_excel(uploaded_file)\n\n# \u8f93\u5165 SQL \u6570\u636e\u5e93\u8fde\u63a5\u4fe1\u606f\ndb_connection = None\n\n#if st.sidebar.button(\"Setup Database Connection\"):\nwith st.sidebar.expander(\"Database Connection Settings\", expanded=True):\n     hostname = st.text_input(\"Hostname\", disabled=uploaded_file is not None)\n     port = st.text_input(\"Port\", disabled=uploaded_file is not None)\n     username = st.text_input(\"Username\", disabled=uploaded_file is not None)\n     password = st.text_input(\"Password\", type=\"password\", disabled=uploaded_file is not None)\n     db_name = st.text_input(\"Database Name\", disabled=uploaded_file is not None)\n     table = st.text_input(\"Table\", disabled=uploaded_file is not None)\n\n#if st.button(\"Connect to SQL Database\"):\n     if hostname and port and username and password and db_name and table:\n        db_connection = MySQLConnector(\n                config={\n                    \"host\": hostname,\n                    \"port\": port,\n                    \"database\": db_name,\n                    \"username\": username,\n                    \"password\": password,\n                    \"table\": table,\n                }\n            )\n\n     if not db_connection:\n        st.error(f\"Error connecting to database: {e}\")\n\n     else:\n        data = db_connection.pandas_df  # \u52a0\u8f7d\u6570\u636e\n\nif data is not None:\n    st.write(data)\n\n</code></pre>"},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#22pandasai","title":"2.2\u7528\u6237\u9009\u62e9pandasai\u4f20\u5165\u7684\u5927\u6a21\u578b","text":"<pre><code>    # \u9009\u62e9\u5927\u8bed\u8a00\u6a21\u578b\n    llm_choice = st.sidebar.selectbox(\"Choose a Language Model\", [\"Ollama\", \"OpenAI\", \"Groq\",\"\u706b\u5c71\"])\n\n    openai_api_key = \"\"\n    groq_api_key = \"\"\n    huoshan_api_key = \"\"\n    huoshan_endid = \"\"\n    if llm_choice == \"OpenAI\":\n        # \u8f93\u5165 OpenAI API Key\n        openai_api_key = st.sidebar.text_input(\"OpenAI API Key\", type=\"password\", disabled=not (uploaded_file or db_connection)) \n        if not openai_api_key.startswith(\"sk-\"):\n            st.warning(\"Please enter your OpenAI API key!\", icon=\"\u26a0\ufe0f\")\n    elif llm_choice == \"Groq\":\n        # \u8f93\u5165 Groq API Key\n        groq_api_key = st.sidebar.text_input(\"Groq API Key\", type=\"password\", disabled=not (uploaded_file or db_connection)) \n        if not groq_api_key.startswith(\"gsk_\"):\n            st.warning(\"Please enter your Groq API key!\", icon=\"\u26a0\ufe0f\")\n    elif llm_choice == \"\u706b\u5c71\":\n        # \u8f93\u5165 Groq API Key\n        huoshan_api_key = st.sidebar.text_input(\"\u706b\u5c71 API KEY\", type=\"password\", disabled=not (uploaded_file or db_connection))\n        huoshan_endid = st.sidebar.text_input(\"\u706b\u5c71 MODEL KEY\", type=\"password\",disabled=not (uploaded_file or db_connection))\n\n\n    def get_llm(choice, api_key=None):\n        if choice == \"OpenAI\":\n            return OpenAI(api_token=api_key)\n        elif choice == \"Groq\":\n            return ChatGroq(model_name='llama3-70b-8192', api_key=api_key)\n        elif choice == \"Ollama\":\n            return Ollama(model=\"llama3.1\")\n        elif choice == \"\u706b\u5c71\":\n            return VolcanoLLM(api_base=\"https://ark.cn-beijing.volces.com/api/v3/chat/completions\",api_key=huoshan_api_key,model=huoshan_endid)\n        else:\n            return None\n</code></pre>"},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#23","title":"2.3\u5bf9\u7528\u6237\u8f93\u5165\u8fdb\u884c\u54cd\u5e94","text":"<pre><code>    def generate_response(data, prompt, llm):\n        df = SmartDataframe(data, config={\n            \"llm\": llm,\n            \"custom_whitelisted_dependencies\": whitelist,\n            \"response_parser\": StreamlitResponse\n        })\n\n        # \u6355\u83b7\u6807\u51c6\u8f93\u51fa\u4ee5\u83b7\u53d6 PandasAI \u7684\u6253\u5370\u5185\u5bb9a\n        old_stdout = sys.stdout\n        sys.stdout = StringIO()\n\n        response = df.chat(prompt)\n        if 'exports/charts/temp_chart.png' in response:\n            image_path = response\n            image = Image.open(image_path)\n            st.image(image, caption='Generated Chart by PandasAI', use_column_width=True)\n        else:\n            st.write(response)\n        st.write(\"All prompts:\")\n        for i, p in enumerate(st.session_state.prompts):\n            st.write(f\"{i}: {p}\")\n</code></pre>"}]}