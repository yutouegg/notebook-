{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"\u6b22\u8fce\u8d56\u5230\u6211\u7684\u5b66\u4e60\u7b14\u8bb0","text":"<p>\u8fd9\u4e2a\u7f51\u7ad9\u662f\u6211\u7684\u5b66\u4e60\u7b14\u8bb0\u4e0e\u5fc3\u5f97</p>"},{"location":"#_2","title":"\u5185\u5bb9\u5bfc\u822a","text":"<ul> <li>Langchain\u5b66\u4e60\u4ee3\u7801\u7b14\u8bb0</li> <li>\u5927\u6a21\u578b\u5fae\u8c03</li> <li>\u6570\u636e\u95ee\u7b54\u754c\u9762</li> <li>\u5927\u6a21\u578b\u539f\u7406</li> </ul>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/","title":"Lora\u5fae\u8c03","text":""},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#1lora","title":"1.\u4ec0\u4e48\u662fLora","text":""},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#11","title":"1.1\u5168\u91cf\u5fae\u8c03\uff1a","text":"<p>\u987e\u540d\u601d\u4e49\uff0c\u5c31\u662f\u628a\u5927\u6a21\u578b\u6240\u6709\u7684\u53c2\u6570\u90fd\u8fdb\u884c\u5fae\u8c03\u3002\u4f46\u662f\u968f\u7740\u5927\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u52a8\u8f84\u5927\u51e0\u5341\u51e0\u767eB\uff0c\u5168\u91cf\u5fae\u8c03\u7684\u538b\u529b\u4e5f\u8d8a\u53d1\u7684\u5927\u3002  </p>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#12","title":"1.2\u66ff\u4ee3\u65b9\u6848\uff1a","text":"<ul> <li>Adapt Tuning\uff1a\u5728\u8bad\u7ec3\u4e2d\u52a0\u5165Adapt\u5c42\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u56fa\u5b9a\u5176\u4ed6\u53c2\u6570\u53ea\u66f4\u65b0Adapt\u5c42\u7684\u53c2\u6570</li> <li>P-tuning\uff1a\u4ed6\u5728\u6bcf\u4e2a\u95ee\u9898\u4e4b\u524d\u6dfb\u52a0\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u524d\u7f00\uff0c\u5c31\u53ef\u4ee5\u7528\u6765\u4e13\u95e8\u8bad\u7ec3\u4e00\u4e2a\u4e13\u4e1a\u6280\u80fd\u3002\u5982\u60f3\u5fae\u8c03\u5927\u6a21\u578b\u6765\u4e13\u95e8\u56de\u7b54\u5386\u53f2\u95ee\u9898\uff0c\u800c\u4e0d\u5e0c\u671b\u4fee\u6539\u6a21\u578b\u7684\u6240\u6709\u53c2\u6570\u3002\u5c31\u53ef\u4ee5\u4f7f\u7528P-tuning\uff0c\u5728\u6bcf\u4e2a\u95ee\u9898\u4e4b\u524d\u6dfb\u52a0\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u524d\u7f00\uff0c\u5982 \u201c\u5386\u53f2\u4e13\u5bb6\uff1a\u201d  </li> </ul>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#13lora","title":"1.3Lora\u5fae\u8c03\uff1a","text":"<p>LoRA \u7684\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u8bad\u7ec3\u4f4e\u79e9\u77e9\u9635\u6355\u6349\u4efb\u52a1\u7279\u5b9a\u7684\u53d8\u5316\uff0c\u800c\u4fdd\u6301\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5927\u90e8\u5206\u53c2\u6570\u4e0d\u53d8\u3002\u4f4e\u79e9\u77e9\u9635\u8868\u793a\u4fe1\u606f\u7684\u538b\u7f29\uff0c\u53ea\u9700\u5c11\u91cf\u53c2\u6570\u5c31\u80fd\u8fd1\u4f3c\u63cf\u8ff0\u6570\u636e\u7684\u4e3b\u8981\u7279\u5f81\u3002 \u5047\u8bbe\u4f60\u6709\u4e00\u5e45\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\u7247\uff08\u9884\u8bad\u7ec3\u6a21\u578b\uff09\uff0c\u4f46\u53ea\u60f3\u8c03\u6574\u989c\u8272\uff08\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\uff09\u3002\u4f60\u4e0d\u9700\u8981\u4fee\u6539\u6bcf\u4e2a\u50cf\u7d20\uff08\u5168\u53c2\u6570\u5fae\u8c03\uff09\uff0c\u800c\u662f\u53ef\u4ee5\u5e94\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u6ee4\u955c\uff08\u4f4e\u79e9\u77e9\u9635\uff09\u6765\u8fbe\u5230\u6548\u679c\u3002\u8fd9\u79cd\u6ee4\u955c\u53ef\u4ee5\u7528\u5f88\u5c11\u7684\u53c2\u6570\u63cf\u8ff0\uff0c\u5374\u80fd\u663e\u8457\u6539\u53d8\u56fe\u7247\u7684\u6574\u4f53\u8272\u8c03\u3002\u8fd9\u5c31\u7c7b\u4f3c\u4e8e LoRA \u7684\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\u3002</p>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#2lorabliblipeft","title":"2.Lora\u5fae\u8c03blibli\u5927\u6a21\u578b(PEFT)\uff1a","text":""},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#21","title":"2.1\u6a21\u578b\u4e0b\u8f7d\uff1a","text":"<pre><code>import torch\nfrom modelscope import snapshot_download, AutoModel, AutoTokenizer\nimport os\n\nmodel_dir = snapshot_download('IndexTeam/Index-1.9B-Chat', cache_dir='model_path', revision='master') #\u7248\u672c\u9009master\u662f\u56e0\u4e3a\u4e00\u822cmaster\u662f\u7a33\u5b9a\u7684\u7248\u672c\n</code></pre>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#22","title":"2.2\u6307\u4ee4\u96c6\u7684\u6784\u5efa","text":"<p>\u8bad\u7ec3\u96c6\u9009\u62e9\u4ece\u7f51\u4e0a\u627e\u5230\u4e00\u4e2a\u5173\u4e8e\u7504\u5b1b\u8bed\u6c14\u7684\u8bad\u7ec3\u96c6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a  </p> <p></p> <p>\u5176\u4e2d\uff0cinstruction \u662f\u7528\u6237\u6307\u4ee4\uff0c\u544a\u77e5\u6a21\u578b\u5176\u9700\u8981\u5b8c\u6210\u7684\u4efb\u52a1\uff1binput \u662f\u7528\u6237\u8f93\u5165\uff0c\u662f\u5b8c\u6210\u7528\u6237\u6307\u4ee4\u6240\u5fc5\u987b\u7684\u8f93\u5165\u5185\u5bb9\uff1boutput \u662f\u6a21\u578b\u5e94\u8be5\u7ed9\u51fa\u7684\u8f93\u51fa\u3002  </p> <p>\u6570\u636e\u683c\u5f0f\u5316\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>def process_func(example):\n    MAX_LENGTH = 384    # \u5206\u8bcd\u5668\u4f1a\u5c06\u4e00\u4e2a\u4e2d\u6587\u5b57\u5207\u5206\u4e3a\u591a\u4e2atoken\uff0c\u56e0\u6b64\u9700\u8981\u653e\u5f00\u4e00\u4e9b\u6700\u5927\u957f\u5ea6\uff0c\u4fdd\u8bc1\u6570\u636e\u7684\u5b8c\u6574\u6027\n    input_ids, attention_mask, labels = [], [], []\n    instruction = tokenizer(f\"&lt;unk&gt;system\u73b0\u5728\u4f60\u8981\u626e\u6f14\u7687\u5e1d\u8eab\u8fb9\u7684\u5973\u4eba--\u7504\u5b1breserved_0user{example['instruction'] + example['input']}reserved_1assistant\", add_special_tokens=False)  # add_special_tokens \u4e0d\u5728\u5f00\u5934\u52a0 special_tokens\n    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # \u56e0\u4e3aeos token\u4e5f\u662f\u8981\u5173\u6ce8\u7684\u6240\u4ee5 \u8865\u5145\u4e3a1\n    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n    if len(input_ids) &gt; MAX_LENGTH:  # \u505a\u4e00\u4e2a\u622a\u65ad\n        input_ids = input_ids[:MAX_LENGTH]\n        attention_mask = attention_mask[:MAX_LENGTH]\n        labels = labels[:MAX_LENGTH]\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n</code></pre> <p>\u8fd9\u4e2a\u8bad\u7ec3\u96c6\u7684\u6784\u5efa\u4e5f\u6709\u70b9\u50cfP\u2014tuning\uff0c\u5728\u8bad\u7ec3\u4e2d\u52a0\u4e86\u524d\u7f00\u201c\u73b0\u5728\u4f60\u8981\u626e\u6f14\u7687\u5e1d\u8eab\u8fb9\u7684\u5973\u4eba--\u7504\u5b1b\u201d process_func\u51fd\u6570\u8fd4\u56de\u4e00\u4e2a\u5b57\u5178\u5305\u542b input_ids\u3001attention_mask \u548c labels\u4e09\u4e2a\u90e8\u5206\uff1a * input_ids\uff1a\u5c06\u8f93\u5165\u7684\u6587\u672c\u7f16\u7801\u4f20\u7ed9\u6a21\u578b * attention_mask\uff1a\u8868\u793a\u6a21\u578b\u9700\u8981\u5173\u6ce8\u54ea\u4e9b\u5730\u65b9\uff0c\u4e00\u822c\u586b\u5145\u7684\u4e0d\u592a\u9700\u8981\u5173\u6ce8 * \u56de\u7b54\u7684\u6587\u672c\u7f16\u7801</p>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#23tokenizer","title":"2.3\u52a0\u8f7d\u6a21\u578b\u548cTokenizer","text":"<pre><code>tokenizer = AutoTokenizer.from_pretrained('/root/autodl-tmp/Tsumugii24/Index-1.9B-Chat/', use_fast=False, trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained('/root/autodl-tmp/Tsumugii24/Index-1.9B-Chat/', device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n</code></pre>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#24loraconfig","title":"2.4\u5b9a\u4e49LoraConfig:","text":"<pre><code>config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM, \n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    inference_mode=False, # \u8bad\u7ec3\u6a21\u5f0f\n    r=8, # \u51b3\u5b9a\u4e86\u4f4e\u79e9\u77e9\u9635\u7684\u7ef4\u5ea6\n    lora_alpha=32, # \u7f29\u653e\u56e0\u5b50\n    lora_dropout=0.1 # Dropout \u6bd4\u4f8b\n)\n</code></pre>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#25train","title":"2.5\u5b9a\u4e49Train\u7684\u53c2\u6570\u5e76\u8bad\u7ec3","text":"<pre><code>args = TrainingArguments(\n    output_dir=\"./output/Index-1.9B-Chat-lora\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    logging_steps=10,\n    num_train_epochs=3,\n    save_steps=100,\n    learning_rate=1e-4,\n    save_on_each_node=True,\n    gradient_checkpointing=True\n   )\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_id,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n)\n\ntrainer.train()\n\n</code></pre> <p>\u540e\u7b49\u5f85\u8bad\u7ec3\u5c31\u597d\u4e86...  </p>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#26lora","title":"2.6\u52a0\u8f7dlora\u5fae\u8c03","text":"<p>\u5fae\u8c03\u5b8c\u6210\u540e\u5f53\u7136\u5c31\u5f97\u8c03\u7528\u5566:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nfrom peft import PeftModel\n\nmodel_path = 'model_path'\nlora_path = 'lora_path'\n\n# \u52a0\u8f7dtokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# \u52a0\u8f7d\u6a21\u578b\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\",torch_dtype=torch.bfloat16)\n\n# \u52a0\u8f7dlora\u6743\u91cd\nmodel = PeftModel.from_pretrained(model, model_id=lora_path, config=config)\n\nprompt = \"\u4f60\u662f\u8c01\uff1f\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"\u73b0\u5728\u4f60\u8981\u626e\u6f14\u7687\u5e1d\u8eab\u8fb9\u7684\u5973\u4eba--\u7504\u5b1b\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\n\ntext = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to('cuda')\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\nprint(response)\n</code></pre> <p>\u8fd9\u5c31\u5b8c\u6210\u4e86\u57fa\u7840\u7684\u5fae\u8c03\u4e86\uff0c1.9B\u7684\u6a21\u578b\u8bad\u7ec3\u96c6\u4e5f\u4e0d\u5927\uff0c\u6211\u57283090\u4e0a\u8bad\u7ec3\u4e0d\u5230\u534a\u4e2a\u5c0f\u65f6\u5c31\u5b8c\u6210\u4e86\u4e00\u6b21\u5fae\u8c03\u3002 \u540e\u7eed\u6211\u4e60\u60ef\u662f\u4f7f\u7528llama.cpp\u5c06\u6a21\u578b\u91cf\u5316\u4f20\u5230ollama\u8fdb\u884c\u8c03\u7528</p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch01_%E6%8F%90%E7%A4%BA%E8%AF%8D/","title":"ch01\u63d0\u793a\u8bcd","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch01_%E6%8F%90%E7%A4%BA%E8%AF%8D/#1","title":"1.\u6a21\u578b\u76f4\u63a5\u56de\u7b54","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch01_%E6%8F%90%E7%A4%BA%E8%AF%8D/#_1","title":"\u5bfc\u5165\u76f8\u5173\u6a21\u5757","text":"<pre><code>import os\nfrom langchain_openai import OpenAI\nfrom langchain_community.llms import HuggingFaceHub\nimport os\nos.environ['OPENAI_API_KEY'] = 'sk-'\n</code></pre>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch01_%E6%8F%90%E7%A4%BA%E8%AF%8D/#_2","title":"\u5bfc\u5165\u6a21\u578b\u8fdb\u884c\u56de\u7b54","text":"<pre><code>llm = OpenAI(\n             temperature=0.9,\n             max_tokens = 256)#temperature\u8d8a\u5c0f\u56de\u7b54\u8d8a\u4e25\u8c28\nprompt = f\"\u4f60\u7684\u7238\u7238\u662f\u8c01\u201c\nresponse = llm(prompt)\n</code></pre>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch01_%E6%8F%90%E7%A4%BA%E8%AF%8D/#2prompt","title":"2.\u7528prompt\u6a21\u5757\u56de\u7b54","text":"<p>\u4f18\u70b9\uff1a\u66f4\u5177\u4e13\u7528\u6027\uff0c\u66f4\u9002\u7528\u4e8e\u4e00\u4e2a\u4e13\u7528\u7684\u7cfb\u7edf</p> <pre><code>restaurant_template = \"\"\"\n\u4f60\u662f\u4e00\u4e2a\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u8bf7\u4f60\u5c06\u4ee5\u4e0b\u6570\u636e{data}\u6309\u503c\u4ece\u5927\u5230\u5c0f\u6392\u5217\n\"\"\"\n\nprompt = PromptTemplate(\n    input_variables=[\"restaurant_desription\"],\n    template=restaurant_template,\n)\n</code></pre>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch02_memery/","title":"ch02\u8bb0\u5fc6","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch02_memery/#1conversationbuffermemory","title":"1.ConversationBufferMemory:","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch02_memery/#_1","title":"\u5bfc\u5165\u76f8\u5173\u6a21\u5757\uff1a","text":"<pre><code>    from langchain.chains.conversation.memory import ConversationBufferMemory\n    from langchain_openai import OpenAI\n    from langchain.chains import ConversationChain\n</code></pre>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch02_memery/#_2","title":"\u8fdb\u884c\u95ee\u7b54\uff1a","text":"<pre><code>    llm = OpenAI( \n             temperature=0, \n             max_tokens = 256)\n     memory = ConversationBufferMemory()\n     conversation = ConversationChain(\n                  llm=llm, \n                  verbose=True, \n                   memory=memory)\n     conversation.predict(input=\"\u4f60\u597d\")\n</code></pre> <p>ConversationBufferMemory\u4f1a\u5c06\u7528\u6237\u4e0eai\u7684\u95ee\u7b54\u548c\u4e0b\u4e00\u6b21\u7684propmt\u4e00\u8d77\u4f20\u7ed9\u5927\u6a21\u578b\u8fdb\u884c\u95ee\u7b54\uff0c\u8fd9\u6837\u7684\u7f3a\u70b9\u5c31\u662f\u5f88\u5feb\u4f1a\u8d85\u51fatoken </p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch02_memery/#2conversationsummarymemory","title":"2.ConversationSummaryMemory","text":"<pre><code>    # memery\u66ff\u6362\u6210ConversationSummaryMemory\n    memory = ConversationSummaryMemory()\n    conversation = ConversationChain(\n                  llm=llm, \n                  verbose=True, \n                   memory=memory)\n     conversation.predict(input=\"\u4f60\u597d\")\n</code></pre> <p>*\u987e\u540d\u601d\u4e49\uff0cConversationSummaryMemory\u4f1a\u5c06\u7528\u6237\u4e0eai\u7684\u5bf9\u8bdd\u8fdb\u884c\u603b\u7ed3\u8fdb\u884c\u8bb0\u5fc6\u518d\u4f20\u7ed9\u5927\u6a21\u578b</p> <ul> <li>\u63d0\u793a\uff1a\u5728langchain0.1.17\u7248\u672c\u4ee5\u540e\u4f7f\u7528langchain.chains\u4f1a\u62a5\u8b66\u544alangchain.chains\u5df2\u7ecf\u4e0d\u518d\u4f7f\u7528</li> </ul>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch03_langchain%E4%BD%BF%E7%94%A8huggingface/","title":"ch03Huggingface","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch03_langchain%E4%BD%BF%E7%94%A8huggingface/#_1","title":"\u901a\u8fc7\u5728\u7ebf\u548c\u672c\u5730\u8c03\u7528\u4e24\u79cd\u65b9\u5f0f\u5206\u522b\u6765\u5b9e\u73b0","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch03_langchain%E4%BD%BF%E7%94%A8huggingface/#1googleflan-t5-xl","title":"1.\u901a\u8fc7\u5728\u7ebf\u7684\u65b9\u5f0f\u8c03\u7528google/flan-t5-xl\u8fdb\u884c\u95ee\u7b54\uff08\u7ed3\u5408\u63d0\u793a\u8bcd\uff09","text":"<pre><code>from langchain_core.prompts import PromptTemplate\nfrom langchain_community.llms import HuggingFaceHub\nfrom langchain.chains import LLMChain\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\n#\u5b9a\u4e49\u94fe\nllm_chain = LLMChain(prompt=prompt, \n                     llm=HuggingFaceHub(repo_id=\"google/flan-t5-xl\", \n                                        model_kwargs={\"temperature\":0, \n                                                      \"max_length\":64}))\nquestion = \"\u5c0f\u7c73\u96c6\u56e2\u7684CEO\u662f\u8c01?\"\n\nprint(llm_chain.invoke(question))\n</code></pre> <p>\u901a\u8fc7\u5728\u7ebf\u8c03\u7528\u7684\u65b9\u5f0f\u53ef\u80fd\u4f1a\u51fa\u73b0\u7f51\u7edc\u548ctoken\u7b49\u5404\u79cd\u83ab\u540d\u5176\u5999\u7684\u9519\u8bef\uff0c\u6240\u4ee5\u4e00\u822c\u6211\u4e0d\u600e\u4e48\u4f7f\u7528huggingface\u5728\u7ebf\u8c03\u7528\u7684\u65b9\u5f0f</p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch03_langchain%E4%BD%BF%E7%94%A8huggingface/#2lacal-model","title":"2.\u4f7f\u7528lacal model","text":"<pre><code>from langchain_community.llms import HuggingFacePipeline\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n\nmodel_id = 'google/flan-t5-large'# go for a smaller model if you dont have the VRAM\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True)#\u4f7f\u75288bit\u91cf\u5316\n\npipe = pipeline(\n    \"text2text-generation\",\n    model=model, \n    tokenizer=tokenizer, \n    max_length=100\n)\n\nlocal_llm = HuggingFacePipeline(pipeline=pipe)\n\nllm_chain = LLMChain(prompt=prompt, \n                     llm=local_llm\n                     )\n\nquestion = \"\u8c01\u662f\u5c0f\u7c73\u96c6\u56e2\u7684CEO?\"\n\nprint(llm_chain.invoke(question))\n</code></pre> <p>\u5f53\u7136\u4f7f\u7528Hugingfacehub\u4e5f\u4e00\u6837\u5f97\u6709Huggingface\u7684key\uff0c\u6211\u901a\u5e38\u559c\u6b22\u7528os\u5c06key\u4fdd\u5b58\u5728\u73af\u5883\u4e2d</p>"},{"location":"%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/","title":"\u4ece0\u6784\u5efaGPT","text":"<p>\u8bad\u7ec3\u96c6\u5728\u8fd9\uff0c\u91c7\u7528\u4e86\u838e\u58eb\u6bd4\u4e9a\u7684\u4f5c\u54c1\u5b50\u96c6\u5f53\u4f5c\u8bad\u7ec3\u96c6\uff0c\u5171111\u4e07\u5b57\u7b26  </p>"},{"location":"%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/#1","title":"1.\u6570\u636e","text":""},{"location":"%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/#11","title":"1.1.\u6570\u636e\u96c6\u89c2\u5bdf\uff1a","text":"<pre><code>chars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n</code></pre> <p>\u4f5c\u7528\u662f\u6253\u5370\u8bcd\u8868\u7684\u5927\u5c0f\uff0c\u8fd0\u884c\u7ed3\u679c\u5982\u4e0b\uff1a  !$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz 65  </p>"},{"location":"%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/#12","title":"1.2.\u6784\u5efa\u7f16\u7801\u5668\u4e0e\u89e3\u7801\u5668\uff1a","text":"<pre><code>stoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] \ndecode = lambda l: ''.join([itos[i] for i in l]) \n\nprint(encode(\"hi I'am here\"))\nprint(decode([46, 47, 1, 21, 5, 39, 51, 1, 46, 43, 56, 43]))\n</code></pre> <p>\u8fd0\u884c\u7ed3\u679c\uff1a [46, 47, 1, 21, 5, 39, 51, 1, 46, 43, 56, 43] hi I'am here \u8fd9\u91cc\u91c7\u7528\u7684\u662f\u6700\u7b80\u5355\u7684\u6309\u7167\u5b57\u7b26\u4f4d\u7f6e\u7f16\u7801\uff0c\u5f53\u7136\u4e2a\u4eba\u89c9\u5f97\u4e5f\u53ef\u4ee5\u518d\u52a0\u7279\u6b8a\u5b57\u7b26\u4ee5\u8868\u793a\u53e5\u5b50\u7684\u5f00\u59cb\u7ed3\u675f  </p>"},{"location":"%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/#13","title":"1.3.\u8bad\u7ec3\u96c6\u7684\u5207\u5206","text":"<pre><code>import torch \ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) \n</code></pre> <p>\u8fd9\u5757\u4ee3\u7801\u5f88\u660e\u663e\u5c31\u662f\u5c06\u6587\u672c\u8f6c\u53d8\u6210tensor\u683c\u5f0f\u7684\u6570\u636e\u4ee5\u4fbf\u8bad\u7ec3  </p> <pre><code>n = int(0.9*len(data)) \ntrain_data = data[:n]\nval_data = data[n:]\n</code></pre> <p>\u5207\u5206\u8bad\u7ec3\u96c6\uff0c\u524d\u767e\u5206\u4e4b\u4e5d\u5341\u4e3a\u8bad\u7ec3\u96c6  </p> <pre><code>block_size = 8\ntrain_data[:block_size+1]\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n</code></pre> <p>\u8bad\u7ec3\u5757\u5927\u5c0f\u5b9a\u4e49\u62108\uff0c\u610f\u601d\u662f1\u63a82\uff0c1\u548c2\u63a83\uff0c\u4ee5\u6b64\u7c7b\u63a8\u3002\u8bad\u7ec3\u5757\u548c\u5f53\u524d\u5927\u6a21\u578b\u6240\u8bf4\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u662f\u4e00\u56de\u4e8b\u3002\u4e2a\u4eba\u66f4\u559c\u6b22\u7406\u89e3\u4e3a\u6ed1\u52a8\u7a97\u53e3\u8bad\u7ec3  </p> <pre><code>def get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n</code></pre> <p>\u5b9a\u4e49\u4e86\u4e00\u4e2a\u968f\u673a\u9009\u53d6\uff08batch\uff0cblock\uff09\u5927\u5c0f\u7684\u6570\u636e  </p>"},{"location":"%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/#2","title":"2.\u6a21\u578b\u6784\u5efa","text":""},{"location":"%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/#21","title":"2.1\u6700\u7b80\u5355\u7684\u4e8c\u5143\u6a21\u578b\uff0c\u635f\u5931\uff0c\u751f\u6210\u7684\u6784\u5efa\uff1a","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        #idx\u548ctarget\u5c31\u53ef\u4ee5\u7406\u89e3\u662f\u4e0a\u9762\u7684xb\u548cyb\uff0c\u4e8c\u8005\u7684\u884c\u72b6\u90fd\u662f\uff08B\uff0cT\uff09\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            logits, loss = self(idx)\n            # \u53ea\u5173\u6ce8\u6700\u540e\u4e00\u4e2a\n            logits = logits[:, -1, :] # becomes (B, C)\n            # \u901a\u8fc7softmax\n            probs = F.softmax(logits, dim=-1) # (B, C)\n\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n</code></pre> <p>\u4ece\u7b80\u5355\u7684\u4e8c\u5143\u6a21\u578b\u5f00\u59cb\u7406\u89e3\uff0cforward\u51fd\u6570\u5c31\u662f\u5c06idx\u901a\u8fc7self.token_embedding_table\u51fd\u6570\u6620\u5c04\u5230\u4e86\u8bcd\u8868\u7ef4\u5ea6\u4e0a\uff0c\u7136\u540e\u6839\u636ecross_entropy\uff08\uff09\u5bf9\u4f20\u5165\u53c2\u6570\u7684\u884c\u72b6\u8981\u6c42\u4f20\u5165\u6570\u636e\u8fd4\u56de\u635f\u5931\u51fd\u6570 generate\uff08\uff09\u51fd\u6570\u5c31\u662f\u521a\u521a\u7406\u89e3\u7684\u6ed1\u52a8\u7a97\u53e3\u5c06\u8f93\u5165\u7684idx\u4f9d\u6b21\u4ece\u8bcd\u8868\u6982\u7387\u4e2d\u901a\u8fc7torch.multinomial\u51fd\u6570\u62bd\u51fa\u4e0b\u4e00\u4e2a\u5355\u8bcd \u751f\u6210\u7ed3\u679c\u5f88\u968f\u673a\uff0c\u8fd9\u6b21\u7684\u8fd0\u884c\u7ed3\u679c\u662f\uff1aSr?qP-QWktXoL&amp;jLDJgOLVz'RIoDqHdhsV&amp;vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3 \u56e0\u4e3amaxtoken\u662f100\uff0c\u4f7f\u7528\u751f\u6210\u4e86\u4e00\u767e\u4e2a\u4e71\u4e03\u516b\u7cdf\u7684\u4e1c\u897f\u3002 \u7ed3\u679c\u5f88\u5dee\uff0c\u5f53\u7136\u6211\u4eec\u4e5f\u53ef\u4ee5\u8bad\u7ec3\u4e00\u4e0b\u6a21\u578b:</p> <pre><code>optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) #\u5b9a\u4e49\u4f18\u5316\u5668\nbatch_size = 32\nfor steps in range(100): # epoch\u662f\u4e00\u767e\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))#\u518d\u770b\u4e00\u4e0b\u7ed3\u679c\n</code></pre> <p>\u5f53\u7136\u8fd9\u53ea\u662f\u4e00\u4e2a\u5c06\u6700\u540e\u4e00\u4e2a\u5b57\u7b26\u63a8\u7406\u901a\u8fc7softmax\u51fd\u6570\u63a8\u6d4b\u4e0b\u4e00\u4e2a\uff0c\u6548\u679c\u80af\u5b9a\u4e00\u822c\u3002\u8fd0\u884c\u7ed3\u679c\u5982\u4e0b\uff1a xiKi-RJ:CgqVuUa!U?qMH.uk!sCuMXvv!CJFfx;LgRyJknOEti.?I&amp;-gPlLyulId?XlaInQ'q,lT$ 3Q&amp;sGlvHQ?mqSq-eON x?SP fUAfCAuCX:bOlgiRQWN:Mphaw tRLKuYXEaAXxrcq-gCUzeh3w!AcyaylgYWjmJM?Uzw:inaY,:C&amp;OECW:vmGGJAn3onAuMgia!ms$Vb q-gCOcPcUhOnxJGUGSPJWT:.?ujmJFoiNYWA'DxY,prZ?qdT;hoo'dHooXXlxf'WkHK&amp;u3Q?rqUi.kz;?Yx?C&amp;u3Qbfzxlyh'Vl:zyxjKXgC?eDT'QKFiBeviNxO'm!Upm$srm&amp;TqViqiBD3HBP!juEOpmZJyF$Fwfy!PlvWPFC &amp;WDdP!Ko,px x tREOE;AJ.BeXkylOVD3KHp$e?nD,.SFbWWI'ubcL!q-tU;aXmJ&amp;uGXHxJXI&amp;Z!gHRpajj;l. pTErIBjx;JKIgoCnLGXrJSP!Ac-AcbczR?    </p> <p>\u589e\u52a0\u8bad\u7ec3\u7684\u6b21\u6570\u5230\u4e00\u4e07\uff0c\u518d\u6b21\u8fd0\u884c\uff1a Iyoteng h hasbe pave pirance Rie hicomyonthar's Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey KIN d pe wither vouprrouthercc. hathe; d! My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha: h hay.JUCle n prids, r loncave w hollular s O: HIs; ht anjx?  </p> <p>DUThinqunt.</p> <p>LaZAnde. athave l. KEONH: ARThanco be y,-hedarwnoddy scace, tridesar, wnl'shenous s ls, theresseys PlorseelapinghiybHen yof GLUCEN t l-t E: I hisgothers je are!-e! QLYotouciullle'z   \u5c31\u53ef\u4ee5\u770b\u5230\u8d77\u7801\u50cf\u6837\u4e86  </p>"},{"location":"%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/#22","title":"2.2\u6784\u5efa\u7279\u5f81","text":"<p>way1\uff1a</p> <pre><code>xbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n</code></pre> <p>\u663e\u800c\u6613\u89c1\u5c31\u662f\u7528\u4e4b\u524d\u91cf\u7684\u5e73\u5747\u503c\u6765\u5b9e\u73b0\u5bf9\u4e4b\u524d\u7279\u5f81\u7684\u63d0\u53d6  </p> <pre><code>wei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x # (B, T, T) @ (B, T, C) ----&gt; (B, T, C)\ntorch.allclose(xbow, xbow2)\n</code></pre> <p>\u8fd9\u662f\u901a\u8fc7\u5411\u91cf\u8ba1\u7b97\u5feb\u901f\u5f97\u5230\u7684\u5e73\u5747\u503c\u7279\u5f81</p> <p>way2(\u91cd\u5934\u620f)\uff1a</p> <pre><code>torch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)   # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x\n\nout.shape\n</code></pre> <p>\u7528\u81ea\u6ce8\u610f\u529b\u6784\u5efa</p>"},{"location":"%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/#3","title":"3.\u5b8c\u6574\u4ee3\u7801\uff1a","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 16 # how many independent sequences will we process in parallel?\nblock_size = 32 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 64\nn_head = 4\nn_layer = 4\ndropout = 0.0\n# ------------\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd) #\u5c06\u591a\u5934\u7ef4\u5ea6\u6620\u5c04\u56de\u53bb\uff0c\u603b\u7ed3\u591a\u5934\u7684\u4f5c\u7528\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd) #\u4f4d\u7f6e\u5d4c\u5165\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) \n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens): #\u57fa\u4e8e\u4fdd\u5b58\u7684\u6a21\u578b\u53c2\u6570\u6765\u751f\u6210\n        for _ in range(max_new_tokens):\n            idx_cond = idx[:, -block_size:]\n            # \u9884\u6d4b\n            logits, loss = self(idx_cond)\n            # \u53ea\u5173\u6ce8\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u6b65\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            #\u7ed3\u679c\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n\n</code></pre>"},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/","title":"\u6570\u636e\u95ee\u7b54","text":""},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#pandasaistreamlit","title":"\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8epandasai\u548cstreamlit\u7684\u6570\u636e\u95ee\u7b54\u7cfb\u7edf","text":""},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#csvexcel","title":"\u7b80\u4ecb\uff1a\u7528\u6237\u53ef\u4ee5\u4f20\u5165csv\u6216excel\u8868\u683c\u6587\u4ef6\uff0c\u53ef\u4ee5\u5bf9\u6570\u636e\u8fdb\u884c\u63d0\u95ee\u3002\u754c\u9762\u5982\u4e0b:","text":"<p>\u8bbe\u8ba1\u7528\u6237\u5171\u6709\u56db\u4e2a\u6a21\u578b\u53ef\u4ee5\u9009\u62e9\uff1a * \u706b\u5c71\u5f15\u64ce * Openai\uff08\u6709\u9700\u8981\u4ee3\u7406\u95ee\u9898\uff09 * ollama(\u5728\u672c\u5730\u6709\u8fd0\u884c\u6162\u7684\u95ee\u9898) * Groq\uff08\u540c\u6837\u6709\u4ee3\u7406\u95ee\u9898\uff09</p>"},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#_1","title":"\u6838\u5fc3\u4ee3\u7801\u89e3\u8bfb:","text":""},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#1","title":"1.\u5bfc\u5165\u6838\u5fc3\u6a21\u5757","text":"<pre><code>import streamlit as st \nimport pandas as pd\nfrom pandasai.llm import OpenAI\nfrom langchain_groq.chat_models import ChatGroq\nfrom langchain_community.llms import Ollama\nfrom pandasai import SmartDataframe, clear_cache\nfrom pandasai.connectors import MySQLConnector\nfrom pandasai.responses.streamlit_response import StreamlitResponse\nfrom io import StringIO, BytesIO\nimport sys\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom huoshan import VolcanoLLM\n\nplt.rcParams['font.sans-serif'] = ['SimHei']  # \u7528\u6765\u6b63\u5e38\u663e\u793a\u4e2d\u6587\u6807\u7b7e\nplt.rcParams['axes.unicode_minus'] = False    # \u7528\u6765\u6b63\u5e38\u663e\u793a\u8d1f\u53f7\n</code></pre>"},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#2","title":"2.\u8bbe\u7f6e\u6807\u9898\uff0c\u548c\u4f20\u5165\u6570\u636e\u6a21\u5757(\u6570\u636e\u53ef\u901a\u8fc7\u4e0a\u4f20\u548c\u6570\u636e\u5e93\u7684\u65b9\u5f0f\u4f20\u5165)","text":"<pre><code>st.set_page_config(page_title=\"Talk to Your Data\")\nst.title(\"Talk to Your Data \ud83d\udc3c\")\n\n# \u4e0a\u4f20 CSV \u6216 Excel \u6587\u4ef6\nuploaded_file = st.sidebar.file_uploader(\"Upload your CSV or Excel file\", type=[\"csv\", \"xlsx\"])\ndata = None\n    if uploaded_file is not None:\n        if uploaded_file.name.endswith(\".csv\"):\n            data = pd.read_csv(uploaded_file)\n        elif uploaded_file.name.endswith(\".xlsx\"):\n            data = pd.read_excel(uploaded_file)\n\n# \u8f93\u5165 SQL \u6570\u636e\u5e93\u8fde\u63a5\u4fe1\u606f\ndb_connection = None\n\n#if st.sidebar.button(\"Setup Database Connection\"):\nwith st.sidebar.expander(\"Database Connection Settings\", expanded=True):\n     hostname = st.text_input(\"Hostname\", disabled=uploaded_file is not None)\n     port = st.text_input(\"Port\", disabled=uploaded_file is not None)\n     username = st.text_input(\"Username\", disabled=uploaded_file is not None)\n     password = st.text_input(\"Password\", type=\"password\", disabled=uploaded_file is not None)\n     db_name = st.text_input(\"Database Name\", disabled=uploaded_file is not None)\n     table = st.text_input(\"Table\", disabled=uploaded_file is not None)\n\n#if st.button(\"Connect to SQL Database\"):\n     if hostname and port and username and password and db_name and table:\n        db_connection = MySQLConnector(\n                config={\n                    \"host\": hostname,\n                    \"port\": port,\n                    \"database\": db_name,\n                    \"username\": username,\n                    \"password\": password,\n                    \"table\": table,\n                }\n            )\n\n     if not db_connection:\n        st.error(f\"Error connecting to database: {e}\")\n\n     else:\n        data = db_connection.pandas_df  # \u52a0\u8f7d\u6570\u636e\n\nif data is not None:\n    st.write(data)\n\n</code></pre>"},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#22pandasai","title":"2.2\u7528\u6237\u9009\u62e9pandasai\u4f20\u5165\u7684\u5927\u6a21\u578b","text":"<pre><code>    # \u9009\u62e9\u5927\u8bed\u8a00\u6a21\u578b\n    llm_choice = st.sidebar.selectbox(\"Choose a Language Model\", [\"Ollama\", \"OpenAI\", \"Groq\",\"\u706b\u5c71\"])\n\n    openai_api_key = \"\"\n    groq_api_key = \"\"\n    huoshan_api_key = \"\"\n    huoshan_endid = \"\"\n    if llm_choice == \"OpenAI\":\n        # \u8f93\u5165 OpenAI API Key\n        openai_api_key = st.sidebar.text_input(\"OpenAI API Key\", type=\"password\", disabled=not (uploaded_file or db_connection)) \n        if not openai_api_key.startswith(\"sk-\"):\n            st.warning(\"Please enter your OpenAI API key!\", icon=\"\u26a0\ufe0f\")\n    elif llm_choice == \"Groq\":\n        # \u8f93\u5165 Groq API Key\n        groq_api_key = st.sidebar.text_input(\"Groq API Key\", type=\"password\", disabled=not (uploaded_file or db_connection)) \n        if not groq_api_key.startswith(\"gsk_\"):\n            st.warning(\"Please enter your Groq API key!\", icon=\"\u26a0\ufe0f\")\n    elif llm_choice == \"\u706b\u5c71\":\n        # \u8f93\u5165 Groq API Key\n        huoshan_api_key = st.sidebar.text_input(\"\u706b\u5c71 API KEY\", type=\"password\", disabled=not (uploaded_file or db_connection))\n        huoshan_endid = st.sidebar.text_input(\"\u706b\u5c71 MODEL KEY\", type=\"password\",disabled=not (uploaded_file or db_connection))\n\n\n    def get_llm(choice, api_key=None):\n        if choice == \"OpenAI\":\n            return OpenAI(api_token=api_key)\n        elif choice == \"Groq\":\n            return ChatGroq(model_name='llama3-70b-8192', api_key=api_key)\n        elif choice == \"Ollama\":\n            return Ollama(model=\"llama3.1\")\n        elif choice == \"\u706b\u5c71\":\n            return VolcanoLLM(api_base=\"https://ark.cn-beijing.volces.com/api/v3/chat/completions\",api_key=huoshan_api_key,model=huoshan_endid)\n        else:\n            return None\n</code></pre>"},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#23","title":"2.3\u5bf9\u7528\u6237\u8f93\u5165\u8fdb\u884c\u54cd\u5e94","text":"<pre><code>    def generate_response(data, prompt, llm):\n        df = SmartDataframe(data, config={\n            \"llm\": llm,\n            \"custom_whitelisted_dependencies\": whitelist,\n            \"response_parser\": StreamlitResponse\n        })\n\n        # \u6355\u83b7\u6807\u51c6\u8f93\u51fa\u4ee5\u83b7\u53d6 PandasAI \u7684\u6253\u5370\u5185\u5bb9a\n        old_stdout = sys.stdout\n        sys.stdout = StringIO()\n\n        response = df.chat(prompt)\n        if 'exports/charts/temp_chart.png' in response:\n            image_path = response\n            image = Image.open(image_path)\n            st.image(image, caption='Generated Chart by PandasAI', use_column_width=True)\n        else:\n            st.write(response)\n        st.write(\"All prompts:\")\n        for i, p in enumerate(st.session_state.prompts):\n            st.write(f\"{i}: {p}\")\n</code></pre>"}]}