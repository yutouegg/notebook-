{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"\u6b22\u8fce\u6765\u5230\u6211\u7684\u5b66\u4e60\u7b14\u8bb0","text":"<p>\u8fd9\u4e2a\u7f51\u7ad9\u662f\u6211\u7684\u5b66\u4e60\u7b14\u8bb0\u4e0e\u5fc3\u5f97</p>"},{"location":"#_2","title":"\u5185\u5bb9\u5bfc\u822a","text":"<ul> <li>Langchain\u5b66\u4e60\u4ee3\u7801\u7b14\u8bb0</li> <li>\u5927\u6a21\u578b\u5fae\u8c03</li> <li>\u6570\u636e\u95ee\u7b54\u754c\u9762</li> <li>\u4ece0\u6784\u5efagpt</li> </ul>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/","title":"Lora\u5fae\u8c03","text":""},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#1lora","title":"1.\u4ec0\u4e48\u662fLora","text":""},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#11","title":"1.1\u5168\u91cf\u5fae\u8c03\uff1a","text":"<p>\u987e\u540d\u601d\u4e49\uff0c\u5c31\u662f\u628a\u5927\u6a21\u578b\u6240\u6709\u7684\u53c2\u6570\u90fd\u8fdb\u884c\u5fae\u8c03\u3002\u4f46\u662f\u968f\u7740\u5927\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u52a8\u8f84\u5927\u51e0\u5341\u51e0\u767eB\uff0c\u5168\u91cf\u5fae\u8c03\u7684\u538b\u529b\u4e5f\u8d8a\u53d1\u7684\u5927\u3002  </p>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#12","title":"1.2\u66ff\u4ee3\u65b9\u6848\uff1a","text":"<ul> <li>Adapt Tuning\uff1a\u5728\u8bad\u7ec3\u4e2d\u52a0\u5165Adapt\u5c42\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u56fa\u5b9a\u5176\u4ed6\u53c2\u6570\u53ea\u66f4\u65b0Adapt\u5c42\u7684\u53c2\u6570</li> <li>P-tuning\uff1a\u4ed6\u5728\u6bcf\u4e2a\u95ee\u9898\u4e4b\u524d\u6dfb\u52a0\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u524d\u7f00\uff0c\u5c31\u53ef\u4ee5\u7528\u6765\u4e13\u95e8\u8bad\u7ec3\u4e00\u4e2a\u4e13\u4e1a\u6280\u80fd\u3002\u5982\u60f3\u5fae\u8c03\u5927\u6a21\u578b\u6765\u4e13\u95e8\u56de\u7b54\u5386\u53f2\u95ee\u9898\uff0c\u800c\u4e0d\u5e0c\u671b\u4fee\u6539\u6a21\u578b\u7684\u6240\u6709\u53c2\u6570\u3002\u5c31\u53ef\u4ee5\u4f7f\u7528P-tuning\uff0c\u5728\u6bcf\u4e2a\u95ee\u9898\u4e4b\u524d\u6dfb\u52a0\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u524d\u7f00\uff0c\u5982 \u201c\u5386\u53f2\u4e13\u5bb6\uff1a\u201d  </li> </ul>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#13lora","title":"1.3Lora\u5fae\u8c03\uff1a","text":"<p>LoRA \u7684\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u8bad\u7ec3\u4f4e\u79e9\u77e9\u9635\u6355\u6349\u4efb\u52a1\u7279\u5b9a\u7684\u53d8\u5316\uff0c\u800c\u4fdd\u6301\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5927\u90e8\u5206\u53c2\u6570\u4e0d\u53d8\u3002\u4f4e\u79e9\u77e9\u9635\u8868\u793a\u4fe1\u606f\u7684\u538b\u7f29\uff0c\u53ea\u9700\u5c11\u91cf\u53c2\u6570\u5c31\u80fd\u8fd1\u4f3c\u63cf\u8ff0\u6570\u636e\u7684\u4e3b\u8981\u7279\u5f81\u3002 \u5047\u8bbe\u4f60\u6709\u4e00\u5e45\u9ad8\u5206\u8fa8\u7387\u7684\u56fe\u7247\uff08\u9884\u8bad\u7ec3\u6a21\u578b\uff09\uff0c\u4f46\u53ea\u60f3\u8c03\u6574\u989c\u8272\uff08\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\uff09\u3002\u4f60\u4e0d\u9700\u8981\u4fee\u6539\u6bcf\u4e2a\u50cf\u7d20\uff08\u5168\u53c2\u6570\u5fae\u8c03\uff09\uff0c\u800c\u662f\u53ef\u4ee5\u5e94\u7528\u4e00\u4e2a\u7b80\u5355\u7684\u6ee4\u955c\uff08\u4f4e\u79e9\u77e9\u9635\uff09\u6765\u8fbe\u5230\u6548\u679c\u3002\u8fd9\u79cd\u6ee4\u955c\u53ef\u4ee5\u7528\u5f88\u5c11\u7684\u53c2\u6570\u63cf\u8ff0\uff0c\u5374\u80fd\u663e\u8457\u6539\u53d8\u56fe\u7247\u7684\u6574\u4f53\u8272\u8c03\u3002\u8fd9\u5c31\u7c7b\u4f3c\u4e8e LoRA \u7684\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\u3002</p>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#2lorabliblipeft","title":"2.Lora\u5fae\u8c03blibli\u5927\u6a21\u578b(PEFT)\uff1a","text":""},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#21","title":"2.1\u6a21\u578b\u4e0b\u8f7d\uff1a","text":"<pre><code>import torch\nfrom modelscope import snapshot_download, AutoModel, AutoTokenizer\nimport os\n\nmodel_dir = snapshot_download('IndexTeam/Index-1.9B-Chat', cache_dir='model_path', revision='master') #\u7248\u672c\u9009master\u662f\u56e0\u4e3a\u4e00\u822cmaster\u662f\u7a33\u5b9a\u7684\u7248\u672c\n</code></pre>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#22","title":"2.2\u6307\u4ee4\u96c6\u7684\u6784\u5efa","text":"<p>\u8bad\u7ec3\u96c6\u9009\u62e9\u4ece\u7f51\u4e0a\u627e\u5230\u4e00\u4e2a\u5173\u4e8e\u7504\u5b1b\u8bed\u6c14\u7684\u8bad\u7ec3\u96c6\uff0c\u5185\u5bb9\u5982\u4e0b\uff1a  </p> <p></p> <p>\u5176\u4e2d\uff0cinstruction \u662f\u7528\u6237\u6307\u4ee4\uff0c\u544a\u77e5\u6a21\u578b\u5176\u9700\u8981\u5b8c\u6210\u7684\u4efb\u52a1\uff1binput \u662f\u7528\u6237\u8f93\u5165\uff0c\u662f\u5b8c\u6210\u7528\u6237\u6307\u4ee4\u6240\u5fc5\u987b\u7684\u8f93\u5165\u5185\u5bb9\uff1boutput \u662f\u6a21\u578b\u5e94\u8be5\u7ed9\u51fa\u7684\u8f93\u51fa\u3002  </p> <p>\u6570\u636e\u683c\u5f0f\u5316\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>def process_func(example):\n    MAX_LENGTH = 384    # \u5206\u8bcd\u5668\u4f1a\u5c06\u4e00\u4e2a\u4e2d\u6587\u5b57\u5207\u5206\u4e3a\u591a\u4e2atoken\uff0c\u56e0\u6b64\u9700\u8981\u653e\u5f00\u4e00\u4e9b\u6700\u5927\u957f\u5ea6\uff0c\u4fdd\u8bc1\u6570\u636e\u7684\u5b8c\u6574\u6027\n    input_ids, attention_mask, labels = [], [], []\n    instruction = tokenizer(f\"&lt;unk&gt;system\u73b0\u5728\u4f60\u8981\u626e\u6f14\u7687\u5e1d\u8eab\u8fb9\u7684\u5973\u4eba--\u7504\u5b1breserved_0user{example['instruction'] + example['input']}reserved_1assistant\", add_special_tokens=False)  # add_special_tokens \u4e0d\u5728\u5f00\u5934\u52a0 special_tokens\n    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # \u56e0\u4e3aeos token\u4e5f\u662f\u8981\u5173\u6ce8\u7684\u6240\u4ee5 \u8865\u5145\u4e3a1\n    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n    if len(input_ids) &gt; MAX_LENGTH:  # \u505a\u4e00\u4e2a\u622a\u65ad\n        input_ids = input_ids[:MAX_LENGTH]\n        attention_mask = attention_mask[:MAX_LENGTH]\n        labels = labels[:MAX_LENGTH]\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n</code></pre> <p>\u8fd9\u4e2a\u8bad\u7ec3\u96c6\u7684\u6784\u5efa\u4e5f\u6709\u70b9\u50cfP\u2014tuning\uff0c\u5728\u8bad\u7ec3\u4e2d\u52a0\u4e86\u524d\u7f00\u201c\u73b0\u5728\u4f60\u8981\u626e\u6f14\u7687\u5e1d\u8eab\u8fb9\u7684\u5973\u4eba--\u7504\u5b1b\u201d process_func\u51fd\u6570\u8fd4\u56de\u4e00\u4e2a\u5b57\u5178\u5305\u542b input_ids\u3001attention_mask \u548c labels\u4e09\u4e2a\u90e8\u5206\uff1a * input_ids\uff1a\u5c06\u8f93\u5165\u7684\u6587\u672c\u7f16\u7801\u4f20\u7ed9\u6a21\u578b * attention_mask\uff1a\u8868\u793a\u6a21\u578b\u9700\u8981\u5173\u6ce8\u54ea\u4e9b\u5730\u65b9\uff0c\u4e00\u822c\u586b\u5145\u7684\u4e0d\u592a\u9700\u8981\u5173\u6ce8 * \u56de\u7b54\u7684\u6587\u672c\u7f16\u7801</p>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#23tokenizer","title":"2.3\u52a0\u8f7d\u6a21\u578b\u548cTokenizer","text":"<pre><code>tokenizer = AutoTokenizer.from_pretrained('/root/autodl-tmp/Tsumugii24/Index-1.9B-Chat/', use_fast=False, trust_remote_code=True)\n\nmodel = AutoModelForCausalLM.from_pretrained('/root/autodl-tmp/Tsumugii24/Index-1.9B-Chat/', device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n</code></pre>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#24loraconfig","title":"2.4\u5b9a\u4e49LoraConfig:","text":"<pre><code>config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM, \n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    inference_mode=False, # \u8bad\u7ec3\u6a21\u5f0f\n    r=8, # \u51b3\u5b9a\u4e86\u4f4e\u79e9\u77e9\u9635\u7684\u7ef4\u5ea6\n    lora_alpha=32, # \u7f29\u653e\u56e0\u5b50\n    lora_dropout=0.1 # Dropout \u6bd4\u4f8b\n)\n</code></pre>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#25train","title":"2.5\u5b9a\u4e49Train\u7684\u53c2\u6570\u5e76\u8bad\u7ec3","text":"<pre><code>args = TrainingArguments(\n    output_dir=\"./output/Index-1.9B-Chat-lora\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    logging_steps=10,\n    num_train_epochs=3,\n    save_steps=100,\n    learning_rate=1e-4,\n    save_on_each_node=True,\n    gradient_checkpointing=True\n   )\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_id,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n)\n\ntrainer.train()\n</code></pre> <p>\u540e\u7b49\u5f85\u8bad\u7ec3\u5c31\u597d\u4e86...  </p>"},{"location":"Lora%E5%BE%AE%E8%B0%83/Lora%E5%BE%AE%E8%B0%83/#26lora","title":"2.6\u52a0\u8f7dlora\u5fae\u8c03","text":"<p>\u5fae\u8c03\u5b8c\u6210\u540e\u5f53\u7136\u5c31\u5f97\u8c03\u7528\u5566:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nfrom peft import PeftModel\n\nmodel_path = 'model_path'\nlora_path = 'lora_path'\n\n# \u52a0\u8f7dtokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# \u52a0\u8f7d\u6a21\u578b\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\",torch_dtype=torch.bfloat16)\n\n# \u52a0\u8f7dlora\u6743\u91cd\nmodel = PeftModel.from_pretrained(model, model_id=lora_path, config=config)\n\nprompt = \"\u4f60\u662f\u8c01\uff1f\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"\u73b0\u5728\u4f60\u8981\u626e\u6f14\u7687\u5e1d\u8eab\u8fb9\u7684\u5973\u4eba--\u7504\u5b1b\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\n\ntext = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to('cuda')\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\nprint(response)\n</code></pre> <p>\u8fd9\u5c31\u5b8c\u6210\u4e86\u57fa\u7840\u7684\u5fae\u8c03\u4e86\uff0c1.9B\u7684\u6a21\u578b\u8bad\u7ec3\u96c6\u4e5f\u4e0d\u5927\uff0c\u6211\u57283090\u4e0a\u8bad\u7ec3\u4e0d\u5230\u534a\u4e2a\u5c0f\u65f6\u5c31\u5b8c\u6210\u4e86\u4e00\u6b21\u5fae\u8c03\u3002 \u540e\u7eed\u6211\u4e60\u60ef\u662f\u4f7f\u7528llama.cpp\u5c06\u6a21\u578b\u91cf\u5316\u4f20\u5230ollama\u8fdb\u884c\u8c03\u7528</p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch01_%E6%8F%90%E7%A4%BA%E8%AF%8D/","title":"ch01\u63d0\u793a\u8bcd","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch01_%E6%8F%90%E7%A4%BA%E8%AF%8D/#1","title":"1.\u6a21\u578b\u76f4\u63a5\u56de\u7b54","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch01_%E6%8F%90%E7%A4%BA%E8%AF%8D/#_1","title":"\u5bfc\u5165\u76f8\u5173\u6a21\u5757","text":"<pre><code>import os\nfrom langchain_openai import OpenAI\nfrom langchain_community.llms import HuggingFaceHub\nimport os\nos.environ['OPENAI_API_KEY'] = 'sk-'\n</code></pre>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch01_%E6%8F%90%E7%A4%BA%E8%AF%8D/#_2","title":"\u5bfc\u5165\u6a21\u578b\u8fdb\u884c\u56de\u7b54","text":"<pre><code>llm = OpenAI(\n             temperature=0.9,\n             max_tokens = 256)#temperature\u8d8a\u5c0f\u56de\u7b54\u8d8a\u4e25\u8c28\nprompt = f\"\u4f60\u7684\u7238\u7238\u662f\u8c01\u201c\nresponse = llm(prompt)\n</code></pre>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch01_%E6%8F%90%E7%A4%BA%E8%AF%8D/#2prompt","title":"2.\u7528prompt\u6a21\u5757\u56de\u7b54","text":"<p>\u4f18\u70b9\uff1a\u66f4\u5177\u4e13\u7528\u6027\uff0c\u66f4\u9002\u7528\u4e8e\u4e00\u4e2a\u4e13\u7528\u7684\u7cfb\u7edf</p> <pre><code>restaurant_template = \"\"\"\n\u4f60\u662f\u4e00\u4e2a\u6570\u636e\u79d1\u5b66\u5bb6\uff0c\u8bf7\u4f60\u5c06\u4ee5\u4e0b\u6570\u636e{data}\u6309\u503c\u4ece\u5927\u5230\u5c0f\u6392\u5217\n\"\"\"\n\nprompt = PromptTemplate(\n    input_variables=[\"restaurant_desription\"],\n    template=restaurant_template,\n)\n</code></pre>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch02_memery/","title":"ch02\u8bb0\u5fc6","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch02_memery/#1conversationbuffermemory","title":"1.ConversationBufferMemory:","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch02_memery/#_1","title":"\u5bfc\u5165\u76f8\u5173\u6a21\u5757\uff1a","text":"<pre><code>    from langchain.chains.conversation.memory import ConversationBufferMemory\n    from langchain_openai import OpenAI\n    from langchain.chains import ConversationChain\n</code></pre>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch02_memery/#_2","title":"\u8fdb\u884c\u95ee\u7b54\uff1a","text":"<pre><code>    llm = OpenAI( \n             temperature=0, \n             max_tokens = 256)\n     memory = ConversationBufferMemory()\n     conversation = ConversationChain(\n                  llm=llm, \n                  verbose=True, \n                   memory=memory)\n     conversation.predict(input=\"\u4f60\u597d\")\n</code></pre> <p>ConversationBufferMemory\u4f1a\u5c06\u7528\u6237\u4e0eai\u7684\u95ee\u7b54\u548c\u4e0b\u4e00\u6b21\u7684propmt\u4e00\u8d77\u4f20\u7ed9\u5927\u6a21\u578b\u8fdb\u884c\u95ee\u7b54\uff0c\u8fd9\u6837\u7684\u7f3a\u70b9\u5c31\u662f\u5f88\u5feb\u4f1a\u8d85\u51fatoken </p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch02_memery/#2conversationsummarymemory","title":"2.ConversationSummaryMemory","text":"<pre><code>    # memery\u66ff\u6362\u6210ConversationSummaryMemory\n    memory = ConversationSummaryMemory()\n    conversation = ConversationChain(\n                  llm=llm, \n                  verbose=True, \n                   memory=memory)\n     conversation.predict(input=\"\u4f60\u597d\")\n</code></pre> <p>*\u987e\u540d\u601d\u4e49\uff0cConversationSummaryMemory\u4f1a\u5c06\u7528\u6237\u4e0eai\u7684\u5bf9\u8bdd\u8fdb\u884c\u603b\u7ed3\u8fdb\u884c\u8bb0\u5fc6\u518d\u4f20\u7ed9\u5927\u6a21\u578b</p> <ul> <li>\u63d0\u793a\uff1a\u5728langchain0.1.17\u7248\u672c\u4ee5\u540e\u4f7f\u7528langchain.chains\u4f1a\u62a5\u8b66\u544alangchain.chains\u5df2\u7ecf\u4e0d\u518d\u4f7f\u7528</li> </ul>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch03_langchain%E4%BD%BF%E7%94%A8huggingface/","title":"ch03Huggingface","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch03_langchain%E4%BD%BF%E7%94%A8huggingface/#_1","title":"\u901a\u8fc7\u5728\u7ebf\u548c\u672c\u5730\u8c03\u7528\u4e24\u79cd\u65b9\u5f0f\u5206\u522b\u6765\u5b9e\u73b0","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch03_langchain%E4%BD%BF%E7%94%A8huggingface/#1googleflan-t5-xl","title":"1.\u901a\u8fc7\u5728\u7ebf\u7684\u65b9\u5f0f\u8c03\u7528google/flan-t5-xl\u8fdb\u884c\u95ee\u7b54\uff08\u7ed3\u5408\u63d0\u793a\u8bcd\uff09","text":"<pre><code>from langchain_core.prompts import PromptTemplate\nfrom langchain_community.llms import HuggingFaceHub\nfrom langchain.chains import LLMChain\n\ntemplate = \"\"\"Question: {question}\n\nAnswer: Let's think step by step.\"\"\"\n\nprompt = PromptTemplate(template=template, input_variables=[\"question\"])\n#\u5b9a\u4e49\u94fe\nllm_chain = LLMChain(prompt=prompt, \n                     llm=HuggingFaceHub(repo_id=\"google/flan-t5-xl\", \n                                        model_kwargs={\"temperature\":0, \n                                                      \"max_length\":64}))\nquestion = \"\u5c0f\u7c73\u96c6\u56e2\u7684CEO\u662f\u8c01?\"\n\nprint(llm_chain.invoke(question))\n</code></pre> <p>\u901a\u8fc7\u5728\u7ebf\u8c03\u7528\u7684\u65b9\u5f0f\u53ef\u80fd\u4f1a\u51fa\u73b0\u7f51\u7edc\u548ctoken\u7b49\u5404\u79cd\u83ab\u540d\u5176\u5999\u7684\u9519\u8bef\uff0c\u6240\u4ee5\u4e00\u822c\u6211\u4e0d\u600e\u4e48\u4f7f\u7528huggingface\u5728\u7ebf\u8c03\u7528\u7684\u65b9\u5f0f</p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch03_langchain%E4%BD%BF%E7%94%A8huggingface/#2lacal-model","title":"2.\u4f7f\u7528lacal model","text":"<pre><code>from langchain_community.llms import HuggingFacePipeline\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n\nmodel_id = 'google/flan-t5-large'# go for a smaller model if you dont have the VRAM\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True)#\u4f7f\u75288bit\u91cf\u5316\n\npipe = pipeline(\n    \"text2text-generation\",\n    model=model, \n    tokenizer=tokenizer, \n    max_length=100\n)\n\nlocal_llm = HuggingFacePipeline(pipeline=pipe)\n\nllm_chain = LLMChain(prompt=prompt, \n                     llm=local_llm\n                     )\n\nquestion = \"\u8c01\u662f\u5c0f\u7c73\u96c6\u56e2\u7684CEO?\"\n\nprint(llm_chain.invoke(question))\n</code></pre> <p>\u5f53\u7136\u4f7f\u7528Hugingfacehub\u4e5f\u4e00\u6837\u5f97\u6709Huggingface\u7684key\uff0c\u6211\u901a\u5e38\u559c\u6b22\u7528os\u5c06key\u4fdd\u5b58\u5728\u73af\u5883\u4e2d</p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch04_langchain_with_pdf/","title":"ch04langchain_pdf","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch04_langchain_with_pdf/#langchainpdf","title":"langchain\u8bfb\u53d6pdf\u5185\u5bb9\u7ed3\u5408\u5927\u6a21\u578b\u8fdb\u884c\u95ee\u7b54","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch04_langchain_with_pdf/#1","title":"1.\u5bfc\u5165\u76f8\u5173\u6a21\u5757\uff1a","text":"<pre><code>from langchain_core.prompts import PromptTemplate\nimport streamlit as st\nfrom PyPDF2 import PdfReader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport os\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\nimport google.generativeai as genai\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain.prompts import PromptTemplate\nfrom dotenv import load_dotenv\n\nload_dotenv()\nos.getenv(\"GOOGLE_API_KEY\")\ngenai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n</code></pre> <p>load_dotenv()\u662f\u4ece.env\u73af\u5883\u4e2d\u8bfb\u53d6GOOGLE_API_KEY\u3002\u5728\u4e0a\u4f20github\u65f6\uff0c.env\u6587\u4ef6\u5e94\u8be5\u653e\u5230gitignre\u6587\u4ef6\u4e2d\u53bb\u3002</p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch04_langchain_with_pdf/#2pdf","title":"2.\u8bfb\u53d6pdf\uff1a","text":"<pre><code>def get_pdf_text(pdf_docs):\n    text=\"\"\n    for pdf in pdf_docs:\n        pdf_reader= PdfReader(pdf)\n        for page in pdf_reader.pages:\n            text+= page.extract_text()\n    return  text\n</code></pre> <p>\u4f7f\u7528\u7684\u662fPyPdf\u4e13\u95e8\u5904\u7406pdf\u7684\u5e93\u3002\u8fd9\u91cc\u4e5f\u53ef\u4ee5\u4f7f\u7528  </p> <pre><code>from langchain_community.document_loaders import PyPDFLoader\n</code></pre> <p>\u7684PyPDFLoader\u8fdb\u884c\u8bfb\u53d6\uff0c\u4e8c\u8005\u6548\u679c\u5dee\u4e0d\u591a\u3002</p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch04_langchain_with_pdf/#3pdf","title":"3.\u5904\u7406pdf\uff1a","text":"<pre><code>def get_text_chunks(text):\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n    chunks = text_splitter.split_text(text)\n    return chunks\n\n\ndef get_vector_store(text_chunks):\n    embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")\n    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n    vector_store.save_local(\"faiss_index\")\n</code></pre> <p>\u8fd9\u662f\u5c06pdf\u7684\u5185\u5bb9\u8fdb\u884c\u5206\u5757\u540e\u5d4c\u5165\u6210\u5411\u91cf\u5b58\u5165\u5411\u91cf\u5e93\u5f53\u4e2d\u53bb\u3002</p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch04_langchain_with_pdf/#4","title":"4.\u5b9a\u4e49\u95ee\u7b54\u903b\u8f91\uff1a","text":"<pre><code>def get_conversational_chain():\n\n    prompt_template = \"\"\"\n    \u6839\u636e\u63d0\u4f9b\u7684\u4e0a\u4e0b\u6587\u5c3d\u53ef\u80fd\u8be6\u7ec6\u5730\u56de\u7b54\u95ee\u9898\uff0c\u786e\u4fdd\u63d0\u4f9b\u6240\u6709\u7ec6\u8282\u3002\u5982\u679c\u7b54\u6848\u4e0d\u5728\u63d0\u4f9b\u7684\u4e0a\u4e0b\u6587\u4e2d\uff0c\u8bf7\u53ea\u8bf4\u201c\u4e0a\u4e0b\u6587\u4e2d\u6ca1\u6709\u53ef\u7528\u7b54\u6848\u201d\uff0c\u4e0d\u8981\u63d0\u4f9b\u9519\u8bef\u7684\u7b54\u6848\u3002/n \u4e0a\u4e0b\u6587\uff1a {context} /n \u95ee\u9898\uff1a {question}\n    Answer:\n    \"\"\"\n\n    model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n                             temperature=0.3)\n\n    prompt = PromptTemplate(template = prompt_template, input_variables = [\"context\", \"question\"])\n    chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n\n    return chain\n\n\n\ndef user_input(user_question):\n    embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")\n\n    new_db = FAISS.load_local(\"faiss_index\", embeddings)\n    docs = new_db.similarity_search(user_question)\n\n    chain = get_conversational_chain()\n\n\n    response = chain(\n        {\"input_documents\":docs, \"question\": user_question}\n        , return_only_outputs=True)\n\n    print(response)\n    st.write(\"Reply: \", response[\"output_text\"])\n</code></pre> <p>\u5c06\u5411\u91cf\u5e93\u4e2d\u7684\u5185\u5bb9\u63d0\u53d6\u51fa\u6765\u5e76\u901a\u8fc7\u63d0\u793a\u8bcd\u8ba9gemini\u56de\u7b54\u7528\u6237\u7684\u95ee\u9898  </p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch04_langchain_with_pdf/#5","title":"5.\u4e3b\u903b\u8f91\uff1a","text":"<pre><code>def main():\n    st.set_page_config(\"Chat PDF\")\n    st.header(\"Chat with PDF using Gemini\ud83d\udc81\")\n\n    user_question = st.text_input(\"Ask a Question from the PDF Files\")\n\n    if user_question:\n        user_input(user_question)\n\n    with st.sidebar:\n        st.title(\"Menu:\")\n        pdf_docs = st.file_uploader(\"Upload your PDF Files and Click on the Submit &amp; Process Button\", accept_multiple_files=True)\n        if st.button(\"Submit &amp; Process\"):\n            with st.spinner(\"Processing...\"):\n                raw_text = get_pdf_text(pdf_docs)\n                text_chunks = get_text_chunks(raw_text)\n                get_vector_store(text_chunks)\n                st.success(\"Done\")\n</code></pre> <p>\u901a\u8fc7streamlit\u505a\u4e00\u4e2a\u5c55\u793a\u9875\u9762\uff0c\u7528\u6237\u4e0a\u4f20pdf\u89e3\u6790\u540e\u6839\u636epdf\u5185\u5bb9\u8fdb\u884c\u63d0\u95ee</p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch05_%E6%96%87%E6%9C%AC%E6%80%BB%E7%BB%93/","title":"ch05\u6587\u672c\u603b\u7ed3","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch05_%E6%96%87%E6%9C%AC%E6%80%BB%E7%BB%93/#langchain","title":"\u4f7f\u7528langchain\u5bf9\u6587\u672c\u8fdb\u884c\u6458\u8981","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch05_%E6%96%87%E6%9C%AC%E6%80%BB%E7%BB%93/#1","title":"1.\u4ec5\u7528\u63d0\u793a\u8bcd\u8fdb\u884c\u6458\u8981\uff1a","text":"<pre><code>from dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAi\nimport os\n\nload_dotenv()\nopenai_key = os.getenv(\"OPENAI_API_KEY\")\nllm = ChatOpenAI(openai_api_key=openai_api_key,model_name='gpt-4o')\n</code></pre> <p>\u521b\u5efa\u63d0\u793a\u8bcd\uff1a</p> <pre><code>from langchain.schema import(\n    AIMessage,\n    HumanMessage,\n    SystemMessage\n)\n\n#\u5b9a\u4e49\u6587\u7ae0\nspeech=\"\"\"\n\u4eba\u5de5\u667a\u80fd\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff1a\u524d\u6cbf\u6280\u672f\u7684\u53d1\u5c55\n\n\u8fd1\u5e74\u6765\uff0c\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5728\u5404\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u5c55\u3002\u6df1\u5ea6\u5b66\u4e60\u662f\u673a\u5668\u5b66\u4e60\u7684\u4e00\u4e2a\u5b50\u9886\u57df\uff0c\u5b83\u4f7f\u7528\u591a\u5c42\u795e\u7ecf\u7f51\u7edc\u6765\u5b66\u4e60\u6570\u636e\u7684\u590d\u6742\u6a21\u5f0f\u3002\u8fd9\u4e9b\u6280\u672f\u5df2\u7ecf\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u56fe\u50cf\u8bc6\u522b\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u3002\n\n\u6df1\u5ea6\u5b66\u4e60\u7684\u57fa\u7840\n\u6df1\u5ea6\u5b66\u4e60\u7684\u6838\u5fc3\u5728\u4e8e\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u7531\u591a\u4e2a\u5c42\u7ec4\u6210\uff0c\u6bcf\u4e00\u5c42\u90fd\u8bd5\u56fe\u5b66\u4e60\u8f93\u5165\u6570\u636e\u7684\u4e0d\u540c\u7279\u5f81\u3002\u5e38\u89c1\u7684\u795e\u7ecf\u7f51\u7edc\u5305\u62ec\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\uff0c\u524d\u8005\u5e38\u7528\u4e8e\u56fe\u50cf\u5904\u7406\uff0c\u540e\u8005\u5219\u5728\u5904\u7406\u5e8f\u5217\u6570\u636e\uff08\u5982\u6587\u672c\uff09\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\n\n\u5e94\u7528\u4e0e\u6311\u6218\n\u4eba\u5de5\u667a\u80fd\u7684\u5e94\u7528\u8303\u56f4\u975e\u5e38\u5e7f\u6cdb\uff0c\u4ece\u667a\u80fd\u8bed\u97f3\u52a9\u624b\u5230\u533b\u7597\u8bca\u65ad\uff0c\u65e0\u5904\u4e0d\u5728\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u53d6\u5f97\u4e86\u8bb8\u591a\u8fdb\u5c55\uff0c\u4eba\u5de5\u667a\u80fd\u6280\u672f\u4ecd\u9762\u4e34\u8bb8\u591a\u6311\u6218\uff0c\u4f8b\u5982\u6a21\u578b\u7684\u900f\u660e\u5ea6\u3001\u6570\u636e\u9690\u79c1\u95ee\u9898\uff0c\u4ee5\u53ca\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u3002\n\n\u672a\u6765\u5c55\u671b\n\u968f\u7740\u8ba1\u7b97\u80fd\u529b\u7684\u63d0\u5347\u548c\u6570\u636e\u91cf\u7684\u589e\u52a0\uff0c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6709\u671b\u5728\u66f4\u591a\u9886\u57df\u53d1\u6325\u4f5c\u7528\u3002\u7814\u7a76\u4eba\u5458\u6b63\u5728\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u548c\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u7684\u6280\u672f\u74f6\u9888\uff0c\u5e76\u8fdb\u4e00\u6b65\u63a8\u8fdb\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u3002\n\"\"\"\n\nchat_messages=[\n    SystemMessage(content='\u4f60\u662f\u4e00\u4e2a\u4e13\u4e1a\u7684\u52a9\u624b\u64c5\u957f\u5bf9\u6587\u7ae0\u8fdb\u884c\u603b\u7ed3'),\n    HumanMessage(content=f'\u8bf7\u5bf9\u4e0b\u9762\u7684\u6587\u7ae0\u8fdb\u884c\u7b80\u77ed\u7684\u6458\u8981\u603b\u7ed3:\\n TEXT: {speech}')\n]\n\nprint(llm.invoke(chat_messages).content)\n</code></pre> <p>\u8fd0\u884c\u7ed3\u679c\u5982\u4e0b\uff1a \u8fd9\u7bc7\u6587\u7ae0\u6982\u8ff0\u4e86\u4eba\u5de5\u667a\u80fd\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u57fa\u672c\u6982\u5ff5\u3001\u5e94\u7528\u9886\u57df\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002\u5982\u679c\u4f60\u6709\u5174\u8da3\u6df1\u5165\u4e86\u89e3\uff0c\u53ef\u4ee5\u67e5\u9605\u76f8\u5173\u7684\u4e13\u4e1a\u4e66\u7c4d\u548c\u5b66\u672f\u8bba\u6587\u3002    </p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch05_%E6%96%87%E6%9C%AC%E6%80%BB%E7%BB%93/#2prompt-templates","title":"2.\u4f7f\u7528Prompt Templates\uff1a","text":"<p>\u8981\u6784\u5efa\u901a\u7528\u7684\u6587\u7ae0\u6458\u8981\u7cfb\u7edf\uff0c\u63d0\u793a\u8bcd\u8fd8\u662f\u9996\u9009Prompt Templates</p> <pre><code>from langchain.chains import LLMChain\nfrom langchain import PromptTemplate\n\ngeneric_template='''\n\u5bf9\u4e0b\u9762\u7684\u6587\u7ae0\u8fdb\u884c\u603b\u7ed3:\nSpeech : `{speech}`\n\u5c06\u603b\u7ed3\u7ffb\u8bd1\u6210\u4e0b\u65b9\u7684\u8bed\u8a00 {language}.\n\n'''\nprompt=PromptTemplate(\n    input_variables=['speech','language'],\n    template=generic_template\n)\n\nprint(prompt.format(speech=speech,language='English'))\n</code></pre> <p>\u8fd0\u884c\u7ed3\u679c\uff1a '\\n\u5bf9\u4e0b\u9762\u7684\u6587\u7ae0\u8fdb\u884c\u603b\u7ed3:\\nSpeech : <code>\\n\u4eba\u5de5\u667a\u80fd\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff1a\u524d\u6cbf\u6280\u672f\u7684\u53d1\u5c55\\n\\n\u8fd1\u5e74\u6765\uff0c\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u5728\u5404\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u5c55\u3002\u6df1\u5ea6\u5b66\u4e60\u662f\u673a\u5668\u5b66\u4e60\u7684\u4e00\u4e2a\u5b50\u9886\u57df\uff0c\u5b83\u4f7f\u7528\u591a\u5c42\u795e\u7ecf\u7f51\u7edc\u6765\u5b66\u4e60\u6570\u636e\u7684\u590d\u6742\u6a21\u5f0f\u3002\u8fd9\u4e9b\u6280\u672f\u5df2\u7ecf\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u56fe\u50cf\u8bc6\u522b\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u3002\\n\\n\u6df1\u5ea6\u5b66\u4e60\u7684\u57fa\u7840\\n\u6df1\u5ea6\u5b66\u4e60\u7684\u6838\u5fc3\u5728\u4e8e\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u7531\u591a\u4e2a\u5c42\u7ec4\u6210\uff0c\u6bcf\u4e00\u5c42\u90fd\u8bd5\u56fe\u5b66\u4e60\u8f93\u5165\u6570\u636e\u7684\u4e0d\u540c\u7279\u5f81\u3002\u5e38\u89c1\u7684\u795e\u7ecf\u7f51\u7edc\u5305\u62ec\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\uff0c\u524d\u8005\u5e38\u7528\u4e8e\u56fe\u50cf\u5904\u7406\uff0c\u540e\u8005\u5219\u5728\u5904\u7406\u5e8f\u5217\u6570\u636e\uff08\u5982\u6587\u672c\uff09\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\\n\\n\u5e94\u7528\u4e0e\u6311\u6218\\n\u4eba\u5de5\u667a\u80fd\u7684\u5e94\u7528\u8303\u56f4\u975e\u5e38\u5e7f\u6cdb\uff0c\u4ece\u667a\u80fd\u8bed\u97f3\u52a9\u624b\u5230\u533b\u7597\u8bca\u65ad\uff0c\u65e0\u5904\u4e0d\u5728\u3002\u7136\u800c\uff0c\u5c3d\u7ba1\u53d6\u5f97\u4e86\u8bb8\u591a\u8fdb\u5c55\uff0c\u4eba\u5de5\u667a\u80fd\u6280\u672f\u4ecd\u9762\u4e34\u8bb8\u591a\u6311\u6218\uff0c\u4f8b\u5982\u6a21\u578b\u7684\u900f\u660e\u5ea6\u3001\u6570\u636e\u9690\u79c1\u95ee\u9898\uff0c\u4ee5\u53ca\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u3002\\n\\n\u672a\u6765\u5c55\u671b\\n\u968f\u7740\u8ba1\u7b97\u80fd\u529b\u7684\u63d0\u5347\u548c\u6570\u636e\u91cf\u7684\u589e\u52a0\uff0c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6709\u671b\u5728\u66f4\u591a\u9886\u57df\u53d1\u6325\u4f5c\u7528\u3002\u7814\u7a76\u4eba\u5458\u6b63\u5728\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u548c\u7b97\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u7684\u6280\u672f\u74f6\u9888\uff0c\u5e76\u8fdb\u4e00\u6b65\u63a8\u8fdb\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u3002\\n</code>\\n\u5c06\u603b\u7ed3\u7ffb\u8bd1\u6210\u4e0b\u65b9\u7684\u8bed\u8a00 English.\\n\\n'</p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch05_%E6%96%87%E6%9C%AC%E6%80%BB%E7%BB%93/#3summarize_chain","title":"3.\u4f7f\u7528summarize_chain\u8fdb\u884c\u603b\u7ed3\uff1a","text":"<pre><code>from PyPDF2 import PdfReader\n\npdfreader = PdfReader(pdf_path)\n\ntext = ''\n#\u5c06\u6bcf\u4e00\u9875\u7684\u6587\u5b57\u8f6c\u6210\u5b57\u7b26\u4e32\u683c\u5f0f\nfor i, page in enumerate(pdfreader.pages):\n    content = page.extract_text()\n    if content:\n        text += content\n\nfrom langchain.docstore.document import Document\ndocs = [Document(page_content=text)]\n</code></pre> <p>\u5c06\u5b57\u7b26\u4e32\u683c\u5f0f\u7684\u6587\u672c\u8f6c\u6210document\u683c\u5f0f\u4e3b\u8981\u662f\u56e0\u4e3a\uff1a  langchin\u901a\u8fc7\u7edf\u4e00\u7684\u63a5\u53e3\u5904\u7406\u5404\u79cd\u6587\u6863\uff0c\u5373\u662fdocument\u3002\u8fd9\u79cd\u4e00\u81f4\u6027\u4f7f\u5f97\u7cfb\u7edf\u66f4\u6613\u4e8e\u6269\u5c55\u548c\u7ef4\u62a4\u3002  Document \u5bf9\u8c61\u901a\u5e38\u5305\u542b\u5143\u6570\u636e\uff0c\u5982\u6587\u6863\u7684\u6807\u9898\u3001\u4f5c\u8005\u3001\u53d1\u5e03\u65e5\u671f\u7b49\u3002\u8fd9\u4e9b\u4fe1\u606f\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u548c\u5904\u7406\u6587\u672c\u5185\u5bb9\u3002  </p> <pre><code>template = '''\u8bf7\u5bf9\u4e0b\u9762\u7684\u6587\u7ae0\u8fdb\u884c\u7b80\u77ed\u7684\u6458\u8981\u603b\u7ed3:\nSpeech: `{text}`\n'''\nprompt = PromptTemplate(\n    input_variables=['text'],\n    template=template\n)\n\nfrom langchain.chains.summarize import load_summarize_chain\n\nchain = load_summarize_chain(\n    llm,\n    chain_type='stuff',\n    prompt=prompt,\n    verbose=False\n)\noutput_summary = chain.invoke(docs)\n</code></pre>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch05_%E6%96%87%E6%9C%AC%E6%80%BB%E7%BB%93/#4map-reduce","title":"4.Map Reduce\u5bf9\u957f\u6587\u672c\u8fdb\u884c\u603b\u7ed3\uff1a","text":"<p>\u5c06\u6587\u672c\u8fdb\u884c\u5206\u8bcd\u9010\u4e2a\u603b\u7ed3\uff1a  </p> <pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\nchunks = text_splitter.create_documents([text])\n\nchain = load_summarize_chain(\n    llm,\n    chain_type='map_reduce',\n    verbose=False\n)\nsummary = chain.run(chunks)\n</code></pre> <p>\u8fd9\u4e2a\u6587\u6863\u7684\u6458\u8981\u8d28\u91cf\u4e0e\u5206\u6bb5\u5927\u5c0f\u548c\u91cd\u53e0\u5927\u5c0f\u7684\u9009\u62e9\u4e5f\u6709\u5f88\u5927\u7684\u5173\u7cfb   </p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch06_LLM%E8%AF%84%E4%BC%B0/","title":"ch06LLM\u8bc4\u4f30","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch06_LLM%E8%AF%84%E4%BC%B0/#_1","title":"\u5927\u6a21\u578b\u8bc4\u4f30","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch06_LLM%E8%AF%84%E4%BC%B0/#1","title":"1.\u5927\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\uff1a","text":"<p>\u5728\u5177\u4f53\u7684\u5927\u6a21\u578b\u5e94\u7528\u5f00\u53d1\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u627e\u5230 Bad Cases\uff0c\u5e76\u4e0d\u65ad\u9488\u5bf9\u6027\u4f18\u5316 Prompt \u6216\u68c0\u7d22\u67b6\u6784\u6765\u89e3\u51b3 Bad Cases\uff0c\u4ece\u800c\u4f18\u5316\u7cfb\u7edf\u7684\u8868\u73b0\u3002\u6211\u4eec\u4f1a\u5c06\u627e\u5230\u7684\u6bcf\u4e00\u4e2a Bad Case \u90fd\u52a0\u5165\u5230\u6211\u4eec\u7684\u9a8c\u8bc1\u96c6\u4e2d\uff0c\u6bcf\u4e00\u6b21\u4f18\u5316\u4e4b\u540e\uff0c\u6211\u4eec\u4f1a\u91cd\u65b0\u5bf9\u9a8c\u8bc1\u96c6\u4e2d\u6240\u6709\u9a8c\u8bc1\u6848\u4f8b\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4ece\u800c\u4fdd\u8bc1\u4f18\u5316\u540e\u7684 \u7cfb\u7edf\u4e0d\u4f1a\u5728\u539f\u6709 Good Case \u4e0a\u5931\u53bb\u80fd\u529b\u6216\u8868\u73b0\u964d\u7ea7\u3002   </p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch06_LLM%E8%AF%84%E4%BC%B0/#1_1","title":"1.\u5f53\u9a8c\u8bc1\u673a\u6bd4\u8f83\u5c0f\uff1a\u4eba\u5de5\u9a8c\u8bc1","text":""},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch06_LLM%E8%AF%84%E4%BC%B0/#11","title":"1.1\u91cf\u5316\u8bc4\u4f30\uff1a","text":"<p>\u5982\u8bc4\u4ef7\u4e0b\u653e\u4e24\u79cd\u65b9\u6848\uff1a</p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch06_LLM%E8%AF%84%E4%BC%B0/#1prompt-a","title":"1.Prompt A\uff1a","text":"<pre><code>from langchain.prompts import PromptTemplate\nfrom langchain.chains import RetrievalQA\n\n\ntemplate_v1 = \"\"\"\u4f7f\u7528\u4ee5\u4e0b\u4e0a\u4e0b\u6587\u6765\u56de\u7b54\u6700\u540e\u7684\u95ee\u9898\u3002\u5982\u679c\u4f60\u4e0d\u77e5\u9053\u7b54\u6848\uff0c\u5c31\u8bf4\u4f60\u4e0d\u77e5\u9053\uff0c\u4e0d\u8981\u8bd5\u56fe\u7f16\u9020\u7b54\n\u6848\u3002\u6700\u591a\u4f7f\u7528\u4e09\u53e5\u8bdd\u3002\u5c3d\u91cf\u4f7f\u7b54\u6848\u7b80\u660e\u627c\u8981\u3002\u603b\u662f\u5728\u56de\u7b54\u7684\u6700\u540e\u8bf4\u201c\u8c22\u8c22\u4f60\u7684\u63d0\u95ee\uff01\u201d\u3002\n{context}\n\u95ee\u9898: {question}\n\"\"\"\n\nQA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],\n                                 template=template_v1)\n\n\n\nqa_chain = RetrievalQA.from_chain_type(llm,\n                                       retriever=vectordb.as_retriever(),\n                                       return_source_documents=True,\n                                       chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})\n\nprint(\"\u95ee\u9898\u4e00\uff1a\")\nquestion = \"\u5357\u74dc\u4e66\u548c\u897f\u74dc\u4e66\u6709\u4ec0\u4e48\u5173\u7cfb\uff1f\"\nresult = qa_chain({\"query\": question})\nprint(result[\"result\"])\n\nprint(\"\u95ee\u9898\u4e8c\uff1a\")\nquestion = \"\u5e94\u8be5\u5982\u4f55\u4f7f\u7528\u5357\u74dc\u4e66\uff1f\"\nresult = qa_chain({\"query\": question})\nprint(result[\"result\"])\n</code></pre> <p>\u7ed3\u679c\uff1a  </p> <pre><code>\u95ee\u9898\u4e00\uff1a  \n\u5357\u74dc\u4e66\u662f\u4ee5\u897f\u74dc\u4e66\u4e3a\u524d\u7f6e\u77e5\u8bc6\u8fdb\u884c\u89e3\u6790\u548c\u8865\u5145\u7684\uff0c\u4e3b\u8981\u662f\u5bf9\u897f\u74dc\u4e66\u4e2d\u6bd4\u8f83\u96be\u7406\u89e3\u7684\u516c\u5f0f\u8fdb\u884c\u89e3\u6790\u548c\u63a8\u5bfc\u7ec6\u8282\u7684\u8865\u5145\u3002\u5357\u74dc\u4e66\u7684\u6700\u4f73\u4f7f\u7528\u65b9\u6cd5\u662f\u4ee5\u897f\u74dc\u4e66\u4e3a\u4e3b\u7ebf\uff0c\u9047\u5230\u81ea\u5df1\u63a8\u5bfc\u4e0d\u51fa\u6765\u6216\u8005\u770b\u4e0d\u61c2\u7684\u516c\u5f0f\u65f6\u518d\u6765\u67e5\u9605\u5357\u74dc\u4e66\u3002\u8c22\u8c22\u4f60\u7684\u63d0\u95ee\uff01  \n\u95ee\u9898\u4e8c\uff1a  \n\u5e94\u8be5\u4ee5\u897f\u74dc\u4e66\u4e3a\u4e3b\u7ebf\uff0c\u9047\u5230\u63a8\u5bfc\u4e0d\u51fa\u6765\u6216\u770b\u4e0d\u61c2\u7684\u516c\u5f0f\u65f6\u518d\u67e5\u9605\u5357\u74dc\u4e66\u3002\u4e0d\u5efa\u8bae\u521d\u5b66\u8005\u6df1\u7a76\u7b2c1\u7ae0\u548c\u7b2c2\u7ae0\u7684\u516c\u5f0f\uff0c\u7b49\u5b66\u5f97\u66f4\u719f\u7ec3\u518d\u56de\u6765\u3002\u5982\u679c\u9700\u8981\u67e5\u9605\u5357\u74dc\u4e66\u4e2d\u6ca1\u6709\u7684\u516c\u5f0f\u6216\u53d1\u73b0\u9519\u8bef\uff0c\u53ef\u4ee5\u5728GitHub\u4e0a\u53cd\u9988\u3002\u8c22\u8c22\u4f60\u7684\u63d0\u95ee! \"\n</code></pre>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch06_LLM%E8%AF%84%E4%BC%B0/#2prompt-b","title":"2.Prompt B:","text":"<pre><code>template_v2 = \"\"\"\u4f7f\u7528\u4ee5\u4e0b\u4e0a\u4e0b\u6587\u6765\u56de\u7b54\u6700\u540e\u7684\u95ee\u9898\u3002\u5982\u679c\u4f60\u4e0d\u77e5\u9053\u7b54\u6848\uff0c\u5c31\u8bf4\u4f60\u4e0d\u77e5\u9053\uff0c\u4e0d\u8981\u8bd5\u56fe\u7f16\u9020\u7b54\n\u6848\u3002\u4f60\u5e94\u8be5\u4f7f\u7b54\u6848\u5c3d\u53ef\u80fd\u8be6\u7ec6\u5177\u4f53\uff0c\u4f46\u4e0d\u8981\u504f\u9898\u3002\u5982\u679c\u7b54\u6848\u6bd4\u8f83\u957f\uff0c\u8bf7\u914c\u60c5\u8fdb\u884c\u5206\u6bb5\uff0c\u4ee5\u63d0\u9ad8\u7b54\u6848\u7684\u9605\u8bfb\u4f53\u9a8c\u3002\n{context}\n\u95ee\u9898: {question}\n\u6709\u7528\u7684\u56de\u7b54:\"\"\"\n\nQA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],\n                                 template=template_v2)\n\nqa_chain = RetrievalQA.from_chain_type(llm,\n                                       retriever=vectordb.as_retriever(),\n                                       return_source_documents=True,\n                                       chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})\n\nprint(\"\u95ee\u9898\u4e00\uff1a\")\nquestion = \"\u5357\u74dc\u4e66\u548c\u897f\u74dc\u4e66\u6709\u4ec0\u4e48\u5173\u7cfb\uff1f\"\nresult = qa_chain({\"query\": question})\nprint(result[\"result\"])\n\nprint(\"\u95ee\u9898\u4e8c\uff1a\")\nquestion = \"\u5e94\u8be5\u5982\u4f55\u4f7f\u7528\u5357\u74dc\u4e66\uff1f\"\nresult = qa_chain({\"query\": question})\nprint(result[\"result\"])\n</code></pre> <p>\u7ed3\u679c:</p> <pre><code>\u95ee\u9898\u4e00\uff1a\n\u5357\u74dc\u4e66\u548c\u897f\u74dc\u4e66\u4e4b\u95f4\u7684\u5173\u7cfb\u662f\u5357\u74dc\u4e66\u662f\u4ee5\u897f\u74dc\u4e66\u7684\u5185\u5bb9\u4e3a\u524d\u7f6e\u77e5\u8bc6\u8fdb\u884c\u8868\u8ff0\u7684\u3002\u5357\u74dc\u4e66\u7684\u76ee\u7684\u662f\u5bf9\u897f\u74dc\u4e66\u4e2d\u6bd4\u8f83\u96be\u7406\u89e3\u7684\u516c\u5f0f\u8fdb\u884c\u89e3\u6790\uff0c\u5e76\u8865\u5145\u5177\u4f53\u7684\u63a8\u5bfc\u7ec6\u8282\uff0c\u4ee5\u5e2e\u52a9\u8bfb\u8005\u66f4\u597d\u5730\u7406\u89e3\u548c\u5b66\u4e60\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u77e5\u8bc6\u3002\u56e0\u6b64\uff0c\u6700\u4f73\u4f7f\u7528\u65b9\u6cd5\u662f\u4ee5\u897f\u74dc\u4e66\u4e3a\u4e3b\u7ebf\uff0c\u9047\u5230\u81ea\u5df1\u63a8\u5bfc\u4e0d\u51fa\u6765\u6216\u8005\u770b\u4e0d\u61c2\u7684\u516c\u5f0f\u65f6\u518d\u6765\u67e5\u9605\u5357\u74dc\u4e66\u3002\u5357\u74dc\u4e66\u7684\u5185\u5bb9\u4e3b\u8981\u662f\u4e3a\u4e86\u5e2e\u52a9\u90a3\u4e9b\u60f3\u6df1\u7a76\u516c\u5f0f\u63a8\u5bfc\u7ec6\u8282\u7684\u8bfb\u8005\uff0c\u63d0\u4f9b\u66f4\u8be6\u7ec6\u7684\u89e3\u91ca\u548c\u8865\u5145\u3002\n\u95ee\u9898\u4e8c\uff1a\n\u5e94\u8be5\u5c06\u5357\u74dc\u4e66\u4f5c\u4e3a\u897f\u74dc\u4e66\u7684\u8865\u5145\uff0c\u4e3b\u8981\u5728\u9047\u5230\u81ea\u5df1\u65e0\u6cd5\u63a8\u5bfc\u6216\u7406\u89e3\u7684\u516c\u5f0f\u65f6\u8fdb\u884c\u67e5\u9605\u3002\u5bf9\u4e8e\u521d\u5b66\u673a\u5668\u5b66\u4e60\u7684\u5c0f\u767d\u6765\u8bf4\uff0c\u5efa\u8bae\u5148\u7b80\u5355\u8fc7\u4e00\u4e0b\u5357\u74dc\u4e66\u7684\u7b2c1\u7ae0\u548c\u7b2c2\u7ae0\u7684\u516c\u5f0f\uff0c\u7b49\u5b66\u5f97\u66f4\u6df1\u5165\u540e\u518d\u56de\u6765\u6df1\u7a76\u3002\u6bcf\u4e2a\u516c\u5f0f\u7684\u89e3\u6790\u548c\u63a8\u5bfc\u90fd\u4ee5\u672c\u79d1\u6570\u5b66\u57fa\u7840\u7684\u89c6\u89d2\u8fdb\u884c\u8bb2\u89e3\uff0c\u8d85\u7eb2\u7684\u6570\u5b66\u77e5\u8bc6\u4f1a\u5728\u9644\u5f55\u548c\u53c2\u8003\u6587\u732e\u4e2d\u7ed9\u51fa\uff0c\u4f9b\u611f\u5174\u8da3\u7684\u540c\u5b66\u7ee7\u7eed\u6df1\u5165\u5b66\u4e60\u3002\u5982\u679c\u5728\u5357\u74dc\u4e66\u4e2d\u627e\u4e0d\u5230\u60f3\u8981\u67e5\u9605\u7684\u516c\u5f0f\uff0c\u6216\u8005\u53d1\u73b0\u9519\u8bef\uff0c\u53ef\u4ee5\u5728GitHub\u7684Issues\u4e2d\u63d0\u4ea4\u53cd\u9988\uff0c\u901a\u5e38\u4f1a\u572824\u5c0f\u65f6\u5185\u5f97\u5230\u56de\u590d\u3002\u6b64\u5916\uff0c\u5357\u74dc\u4e66\u8fd8\u63d0\u4f9b\u914d\u5957\u89c6\u9891\u6559\u7a0b\u548c\u5728\u7ebf\u9605\u8bfb\u5730\u5740\uff0c\u4ee5\u53ca\u6700\u65b0\u7248PDF\u83b7\u53d6\u5730\u5740\u3002\u6700\u540e\uff0c\u5357\u74dc\u4e66\u7684\u5185\u5bb9\u662f\u57fa\u4e8e\u77e5\u8bc6\u5171\u4eab\u7f72\u540d-\u975e\u5546\u4e1a\u6027\u4f7f\u7528-\u76f8\u540c\u65b9\u5f0f\u5171\u4eab4.0\u56fd\u9645\u8bb8\u53ef\u534f\u8bae\u8fdb\u884c\u8bb8\u53ef\u3002\n</code></pre> <p>\u5f88\u597d\u7406\u89e3\uff0c\u5047\u8bbe\u8bc4\u5206\u89c4\u5219\u662f1-5\u5206\uff0cA\u7684\u4e00\u95ee\u9898\u5f97\u5206\u4e3a4\uff0c\u4e8c\u95ee\u9898\u5f97\u5206\u4e3a2\u3002B\u7684\u95ee\u9898\u4e00\u5f97\u5206\u4e3a3\uff0c\u4e8c\u95ee\u9898\u5f97\u5206\u4e3a4.\u8fd9\u6837\u5e73\u5747\u5f97\u5206B\u5927\u4e8eA\uff0c\u5219\u5224\u65adB\u65b9\u6848\u66f4\u597d  </p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch06_LLM%E8%AF%84%E4%BC%B0/#12","title":"1.2\u591a\u7ef4\u8bc4\u5206\uff1a","text":"<p>\u4f8b\u5982\uff0c\u8981\u8bc4\u4ef7\u4e00\u4e2aRAG\u7cfb\u7edf\uff1a \u2460 \u77e5\u8bc6\u67e5\u627e\u6b63\u786e\u6027\u3002\u8be5\u7ef4\u5ea6\u9700\u8981\u67e5\u770b\u7cfb\u7edf\u4ece\u5411\u91cf\u6570\u636e\u5e93\u67e5\u627e\u76f8\u5173\u77e5\u8bc6\u7247\u6bb5\u7684\u4e2d\u95f4\u7ed3\u679c\uff0c\u8bc4\u4f30\u7cfb\u7edf\u67e5\u627e\u5230\u7684\u77e5\u8bc6\u7247\u6bb5\u662f\u5426\u80fd\u591f\u5bf9\u95ee\u9898\u505a\u51fa\u56de\u7b54\u3002\u8be5\u7ef4\u5ea6\u4e3a0-1\u8bc4\u4f30\uff0c\u5373\u6253\u5206\u4e3a0\u6307\u67e5\u627e\u5230\u7684\u77e5\u8bc6\u7247\u6bb5\u4e0d\u80fd\u505a\u51fa\u56de\u7b54\uff0c\u6253\u5206\u4e3a1\u6307\u67e5\u627e\u5230\u7684\u77e5\u8bc6\u7247\u6bb5\u53ef\u4ee5\u505a\u51fa\u56de\u7b54\u3002</p> <p>\u2461 \u56de\u7b54\u4e00\u81f4\u6027\u3002\u8be5\u7ef4\u5ea6\u8bc4\u4f30\u7cfb\u7edf\u7684\u56de\u7b54\u662f\u5426\u9488\u5bf9\u7528\u6237\u95ee\u9898\u5c55\u5f00\uff0c\u662f\u5426\u6709\u504f\u9898\u3001\u9519\u8bef\u7406\u89e3\u9898\u610f\u7684\u60c5\u51b5\uff0c\u8be5\u7ef4\u5ea6\u91cf\u7eb2\u540c\u6837\u8bbe\u8ba1\u4e3a0~1\uff0c0\u4e3a\u5b8c\u5168\u504f\u9898\uff0c1\u4e3a\u5b8c\u5168\u5207\u9898\uff0c\u4e2d\u95f4\u7ed3\u679c\u53ef\u4ee5\u4efb\u53d6\u3002</p> <p>\u2462 \u56de\u7b54\u5e7b\u89c9\u6bd4\u4f8b\u3002\u8be5\u7ef4\u5ea6\u9700\u8981\u7efc\u5408\u7cfb\u7edf\u56de\u7b54\u4e0e\u67e5\u627e\u5230\u7684\u77e5\u8bc6\u7247\u6bb5\uff0c\u8bc4\u4f30\u7cfb\u7edf\u7684\u56de\u7b54\u662f\u5426\u51fa\u73b0\u5e7b\u89c9\uff0c\u5e7b\u89c9\u6bd4\u4f8b\u6709\u591a\u9ad8\u3002\u8be5\u7ef4\u5ea6\u540c\u6837\u8bbe\u8ba1\u4e3a0~1,0\u4e3a\u5168\u90e8\u662f\u6a21\u578b\u5e7b\u89c9\uff0c1\u4e3a\u6ca1\u6709\u4efb\u4f55\u5e7b\u89c9\u3002</p> <p>\u2463 \u56de\u7b54\u6b63\u786e\u6027\u3002\u8be5\u7ef4\u5ea6\u8bc4\u4f30\u7cfb\u7edf\u56de\u7b54\u662f\u5426\u6b63\u786e\uff0c\u662f\u5426\u5145\u5206\u89e3\u7b54\u4e86\u7528\u6237\u95ee\u9898\uff0c\u662f\u7cfb\u7edf\u6700\u6838\u5fc3\u7684\u8bc4\u4f30\u6307\u6807\u4e4b\u4e00\u3002\u8be5\u7ef4\u5ea6\u53ef\u4ee5\u57280~1\u4e4b\u95f4\u4efb\u610f\u6253\u5206\u3002</p> <p>\u4e0a\u8ff0\u56db\u4e2a\u7ef4\u5ea6\u90fd\u56f4\u7ed5\u77e5\u8bc6\u3001\u56de\u7b54\u7684\u6b63\u786e\u6027\u5c55\u5f00\uff0c\u4e0e\u95ee\u9898\u9ad8\u5ea6\u76f8\u5173\uff1b\u63a5\u4e0b\u6765\u51e0\u4e2a\u7ef4\u5ea6\u5c06\u56f4\u7ed5\u5927\u6a21\u578b\u751f\u6210\u7ed3\u679c\u7684\u62df\u4eba\u6027\u3001\u8bed\u6cd5\u6b63\u786e\u6027\u5c55\u5f00\uff0c\u4e0e\u95ee\u9898\u76f8\u5173\u6027\u8f83\u5c0f\uff1a</p> <p>\u2464 \u903b\u8f91\u6027\u3002\u8be5\u7ef4\u5ea6\u8bc4\u4f30\u7cfb\u7edf\u56de\u7b54\u662f\u5426\u903b\u8f91\u8fde\u8d2f\uff0c\u662f\u5426\u51fa\u73b0\u524d\u540e\u51b2\u7a81\u3001\u903b\u8f91\u6df7\u4e71\u7684\u60c5\u51b5\u3002\u8be5\u7ef4\u5ea6\u4e3a0-1\u8bc4\u4f30\u3002</p> <p>\u2465 \u901a\u987a\u6027\u3002\u8be5\u7ef4\u5ea6\u8bc4\u4f30\u7cfb\u7edf\u56de\u7b54\u662f\u5426\u901a\u987a\u3001\u5408\u4e4e\u8bed\u6cd5\uff0c\u53ef\u4ee5\u57280~1\u4e4b\u95f4\u4efb\u610f\u6253\u5206\u3002</p> <p>\u2466 \u667a\u80fd\u6027\u3002\u8be5\u7ef4\u5ea6\u8bc4\u4f30\u7cfb\u7edf\u56de\u7b54\u662f\u5426\u62df\u4eba\u5316\u3001\u667a\u80fd\u5316\uff0c\u662f\u5426\u80fd\u5145\u5206\u8ba9\u7528\u6237\u6df7\u6dc6\u4eba\u5de5\u56de\u7b54\u4e0e\u667a\u80fd\u56de\u7b54\u3002\u8be5\u7ef4\u5ea6\u53ef\u4ee5\u57280~1\u4e4b\u95f4\u4efb\u610f\u6253\u5206\u3002</p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch06_LLM%E8%AF%84%E4%BC%B0/#2","title":"2.\u81ea\u52a8\u8bc4\u4f30\uff1a","text":"<p>\u5927\u6a21\u578b\u81ea\u52a8\u8bc4\u4f30\u7684\u96be\u70b9\u5728\u4e8e\u5ba2\u89c2\u9898\uff0c\u6ca1\u6709\u6807\u51c6\u7684\u7b54\u6848\uff1a</p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch06_LLM%E8%AF%84%E4%BC%B0/#21","title":"2.1\u6784\u5efa\u5ba2\u89c2\u9898\uff1a","text":"<pre><code>prompt_template = '''\n\u8bf7\u4f60\u505a\u5982\u4e0b\u9009\u62e9\u9898\uff1a\n\u9898\u76ee\uff1a\u5357\u74dc\u4e66\u7684\u4f5c\u8005\u662f\u8c01\uff1f\n\u9009\u9879\uff1aA \u5468\u5fd7\u660e B \u8c22\u6587\u777f C \u79e6\u5dde D \u8d3e\u5f6c\u5f6c\n\u4f60\u53ef\u4ee5\u53c2\u8003\u7684\u77e5\u8bc6\u7247\u6bb5\uff1a\n~~~\n{}\n~~~\n\u8bf7\u4ec5\u8fd4\u56de\u9009\u62e9\u7684\u9009\u9879\n\u5982\u679c\u4f60\u65e0\u6cd5\u505a\u51fa\u9009\u62e9\uff0c\u8bf7\u8fd4\u56de\u7a7a\n'''\n</code></pre> <pre><code>def multi_select_score_v2(true_answer : str, generate_answer : str) -&gt; float:\n    # true_anser : \u6b63\u786e\u7b54\u6848\uff0cstr \u7c7b\u578b\uff0c\u4f8b\u5982 'BCD'\n    # generate_answer : \u6a21\u578b\u751f\u6210\u7b54\u6848\uff0cstr \u7c7b\u578b\n    true_answers = list(true_answer)\n    '''\u4e3a\u4fbf\u4e8e\u8ba1\u7b97\uff0c\u6211\u4eec\u5047\u8bbe\u6bcf\u9053\u9898\u90fd\u53ea\u6709 A B C D \u56db\u4e2a\u9009\u9879'''\n    # \u5148\u627e\u51fa\u9519\u8bef\u7b54\u6848\u96c6\u5408\n    false_answers = [item for item in ['A', 'B', 'C', 'D'] if item not in true_answers]\n    # \u5982\u679c\u751f\u6210\u7b54\u6848\u51fa\u73b0\u4e86\u9519\u8bef\u7b54\u6848\n    for one_answer in false_answers:\n        if one_answer in generate_answer:\n            return -1\n    # \u518d\u5224\u65ad\u662f\u5426\u5168\u9009\u4e86\u6b63\u786e\u7b54\u6848\n    if_correct = 0\n    for one_answer in true_answers:\n        if one_answer in generate_answer:\n            if_correct += 1\n            continue\n    if if_correct == 0:\n        # \u4e0d\u9009\n        return 0\n    elif if_correct == len(true_answers):\n        # \u5168\u9009\n        return 1\n    else:\n        # \u6f0f\u9009\n        return 0.5\n</code></pre> <p>\u901a\u8fc7\u5ba2\u89c2\u9898\u8fdb\u884c\u6253\u5206</p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch06_LLM%E8%AF%84%E4%BC%B0/#22","title":"2.2\u8ba1\u7b97\u7b54\u6848\u76f8\u4f3c\u5ea6","text":"<p>\u8be5\u65b9\u6848\u9700\u8981\u4eba\u5de5\u6784\u5efa\u6807\u51c6\u7b54\u6848\uff0c\u518d\u901a\u8fc7\u8ba1\u7b97\u76f8\u4f3c\u6027\u6253\u5206\uff08\u4e3b\u8981\u7528\u4e8e\u4e3b\u89c2\u9898\uff09  </p>"},{"location":"langchain%E5%AD%A6%E4%B9%A0/ch06_LLM%E8%AF%84%E4%BC%B0/#23","title":"2.3\u4f7f\u7528\u5927\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff1a","text":"<p>\u4f7f\u7528\u5927\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u4e00\u4e2a\u6700\u57fa\u672c\u7684\u51c6\u5219\u5c31\u662f\u8bc4\u4ef7\u7684\u5927\u6a21\u578b\u6027\u80fd\u8981\u597d\u4e8e\u57fa\u5ea7\u5927\u5927\u6a21\u578b \u4f8b\u5982\uff0c\u6211\u4eec\u53ef\u4ee5\u6784\u9020\u5982\u4e0b\u7684 Prompt Engineering\uff0c\u8ba9\u5927\u6a21\u578b\u8fdb\u884c\u6253\u5206\uff1a   </p> <pre><code>prompt = '''\n\u4f60\u662f\u4e00\u4e2a\u6a21\u578b\u56de\u7b54\u8bc4\u4f30\u5458\u3002\n\u63a5\u4e0b\u6765\uff0c\u6211\u5c06\u7ed9\u4f60\u4e00\u4e2a\u95ee\u9898\u3001\u5bf9\u5e94\u7684\u77e5\u8bc6\u7247\u6bb5\u4ee5\u53ca\u6a21\u578b\u6839\u636e\u77e5\u8bc6\u7247\u6bb5\u5bf9\u95ee\u9898\u7684\u56de\u7b54\u3002\n\u8bf7\u4f60\u4f9d\u6b21\u8bc4\u4f30\u4ee5\u4e0b\u7ef4\u5ea6\u6a21\u578b\u56de\u7b54\u7684\u8868\u73b0\uff0c\u5206\u522b\u7ed9\u51fa\u6253\u5206\uff1a\n\n\u2460 \u77e5\u8bc6\u67e5\u627e\u6b63\u786e\u6027\u3002\u8bc4\u4f30\u7cfb\u7edf\u7ed9\u5b9a\u7684\u77e5\u8bc6\u7247\u6bb5\u662f\u5426\u80fd\u591f\u5bf9\u95ee\u9898\u505a\u51fa\u56de\u7b54\u3002\u5982\u679c\u77e5\u8bc6\u7247\u6bb5\u4e0d\u80fd\u505a\u51fa\u56de\u7b54\uff0c\u6253\u5206\u4e3a0\uff1b\u5982\u679c\u77e5\u8bc6\u7247\u6bb5\u53ef\u4ee5\u505a\u51fa\u56de\u7b54\uff0c\u6253\u5206\u4e3a1\u3002\n\n\u2461 \u56de\u7b54\u4e00\u81f4\u6027\u3002\u8bc4\u4f30\u7cfb\u7edf\u7684\u56de\u7b54\u662f\u5426\u9488\u5bf9\u7528\u6237\u95ee\u9898\u5c55\u5f00\uff0c\u662f\u5426\u6709\u504f\u9898\u3001\u9519\u8bef\u7406\u89e3\u9898\u610f\u7684\u60c5\u51b5\uff0c\u6253\u5206\u5206\u503c\u57280~1\u4e4b\u95f4\uff0c0\u4e3a\u5b8c\u5168\u504f\u9898\uff0c1\u4e3a\u5b8c\u5168\u5207\u9898\u3002\n\n\u2462 \u56de\u7b54\u5e7b\u89c9\u6bd4\u4f8b\u3002\u8be5\u7ef4\u5ea6\u9700\u8981\u7efc\u5408\u7cfb\u7edf\u56de\u7b54\u4e0e\u67e5\u627e\u5230\u7684\u77e5\u8bc6\u7247\u6bb5\uff0c\u8bc4\u4f30\u7cfb\u7edf\u7684\u56de\u7b54\u662f\u5426\u51fa\u73b0\u5e7b\u89c9\uff0c\u6253\u5206\u5206\u503c\u57280~1\u4e4b\u95f4,0\u4e3a\u5168\u90e8\u662f\u6a21\u578b\u5e7b\u89c9\uff0c1\u4e3a\u6ca1\u6709\u4efb\u4f55\u5e7b\u89c9\u3002\n\n\u2463 \u56de\u7b54\u6b63\u786e\u6027\u3002\u8be5\u7ef4\u5ea6\u8bc4\u4f30\u7cfb\u7edf\u56de\u7b54\u662f\u5426\u6b63\u786e\uff0c\u662f\u5426\u5145\u5206\u89e3\u7b54\u4e86\u7528\u6237\u95ee\u9898\uff0c\u6253\u5206\u5206\u503c\u57280~1\u4e4b\u95f4\uff0c0\u4e3a\u5b8c\u5168\u4e0d\u6b63\u786e\uff0c1\u4e3a\u5b8c\u5168\u6b63\u786e\u3002\n\n\u2464 \u903b\u8f91\u6027\u3002\u8be5\u7ef4\u5ea6\u8bc4\u4f30\u7cfb\u7edf\u56de\u7b54\u662f\u5426\u903b\u8f91\u8fde\u8d2f\uff0c\u662f\u5426\u51fa\u73b0\u524d\u540e\u51b2\u7a81\u3001\u903b\u8f91\u6df7\u4e71\u7684\u60c5\u51b5\u3002\u6253\u5206\u5206\u503c\u57280~1\u4e4b\u95f4\uff0c0\u4e3a\u903b\u8f91\u5b8c\u5168\u6df7\u4e71\uff0c1\u4e3a\u5b8c\u5168\u6ca1\u6709\u903b\u8f91\u95ee\u9898\u3002\n\n\u2465 \u901a\u987a\u6027\u3002\u8be5\u7ef4\u5ea6\u8bc4\u4f30\u7cfb\u7edf\u56de\u7b54\u662f\u5426\u901a\u987a\u3001\u5408\u4e4e\u8bed\u6cd5\u3002\u6253\u5206\u5206\u503c\u57280~1\u4e4b\u95f4\uff0c0\u4e3a\u8bed\u53e5\u5b8c\u5168\u4e0d\u901a\u987a\uff0c1\u4e3a\u8bed\u53e5\u5b8c\u5168\u901a\u987a\u6ca1\u6709\u4efb\u4f55\u8bed\u6cd5\u95ee\u9898\u3002\n\n\u2466 \u667a\u80fd\u6027\u3002\u8be5\u7ef4\u5ea6\u8bc4\u4f30\u7cfb\u7edf\u56de\u7b54\u662f\u5426\u62df\u4eba\u5316\u3001\u667a\u80fd\u5316\uff0c\u662f\u5426\u80fd\u5145\u5206\u8ba9\u7528\u6237\u6df7\u6dc6\u4eba\u5de5\u56de\u7b54\u4e0e\u667a\u80fd\u56de\u7b54\u3002\u6253\u5206\u5206\u503c\u57280~1\u4e4b\u95f4\uff0c0\u4e3a\u975e\u5e38\u660e\u663e\u7684\u6a21\u578b\u56de\u7b54\uff0c1\u4e3a\u4e0e\u4eba\u5de5\u56de\u7b54\u9ad8\u5ea6\u4e00\u81f4\u3002\n\n\u4f60\u5e94\u8be5\u662f\u6bd4\u8f83\u4e25\u82db\u7684\u8bc4\u4f30\u5458\uff0c\u5f88\u5c11\u7ed9\u51fa\u6ee1\u5206\u7684\u9ad8\u8bc4\u4f30\u3002\n\u7528\u6237\u95ee\u9898\uff1a\n~~~\n{}\n~~~\n\u5f85\u8bc4\u4f30\u7684\u56de\u7b54\uff1a\n~~~\n{}\n~~~\n\u7ed9\u5b9a\u7684\u77e5\u8bc6\u7247\u6bb5\uff1a\n~~~\n{}\n~~~\n\u4f60\u5e94\u8be5\u8fd4\u56de\u7ed9\u6211\u4e00\u4e2a\u53ef\u76f4\u63a5\u89e3\u6790\u7684 Python \u5b57\u5178\uff0c\u5b57\u5178\u7684\u952e\u662f\u5982\u4e0a\u7ef4\u5ea6\uff0c\u503c\u662f\u6bcf\u4e00\u4e2a\u7ef4\u5ea6\u5bf9\u5e94\u7684\u8bc4\u4f30\u6253\u5206\u3002\n\u4e0d\u8981\u8f93\u51fa\u4efb\u4f55\u5176\u4ed6\u5185\u5bb9\u3002\n'''\n</code></pre> <p>\u5927\u6a21\u578b\u8bc4\u4ef7\u53ef\u4ee5\u4ece\u591a\u7ef4\u5ea6\u8bc4\u4ef7\u5165\u624b</p>"},{"location":"%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/","title":"\u4ece0\u6784\u5efaGPT","text":"<p>\u8bad\u7ec3\u96c6\u5728\u8fd9\uff0c\u91c7\u7528\u4e86\u838e\u58eb\u6bd4\u4e9a\u7684\u4f5c\u54c1\u5b50\u96c6\u5f53\u4f5c\u8bad\u7ec3\u96c6\uff0c\u5171111\u4e07\u5b57\u7b26  </p>"},{"location":"%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/#1","title":"1.\u6570\u636e","text":""},{"location":"%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/#11","title":"1.1.\u6570\u636e\u96c6\u89c2\u5bdf\uff1a","text":"<pre><code>chars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n</code></pre> <p>\u4f5c\u7528\u662f\u6253\u5370\u8bcd\u8868\u7684\u5927\u5c0f\uff0c\u8fd0\u884c\u7ed3\u679c\u5982\u4e0b\uff1a  !$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz 65  </p>"},{"location":"%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/#12","title":"1.2.\u6784\u5efa\u7f16\u7801\u5668\u4e0e\u89e3\u7801\u5668\uff1a","text":"<pre><code>stoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] \ndecode = lambda l: ''.join([itos[i] for i in l]) \n\nprint(encode(\"hi I'am here\"))\nprint(decode([46, 47, 1, 21, 5, 39, 51, 1, 46, 43, 56, 43]))\n</code></pre> <p>\u8fd0\u884c\u7ed3\u679c\uff1a [46, 47, 1, 21, 5, 39, 51, 1, 46, 43, 56, 43] hi I'am here \u8fd9\u91cc\u91c7\u7528\u7684\u662f\u6700\u7b80\u5355\u7684\u6309\u7167\u5b57\u7b26\u4f4d\u7f6e\u7f16\u7801\uff0c\u5f53\u7136\u4e2a\u4eba\u89c9\u5f97\u4e5f\u53ef\u4ee5\u518d\u52a0\u7279\u6b8a\u5b57\u7b26\u4ee5\u8868\u793a\u53e5\u5b50\u7684\u5f00\u59cb\u7ed3\u675f  </p>"},{"location":"%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/#13","title":"1.3.\u8bad\u7ec3\u96c6\u7684\u5207\u5206","text":"<pre><code>import torch \ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) \n</code></pre> <p>\u8fd9\u5757\u4ee3\u7801\u5f88\u660e\u663e\u5c31\u662f\u5c06\u6587\u672c\u8f6c\u53d8\u6210tensor\u683c\u5f0f\u7684\u6570\u636e\u4ee5\u4fbf\u8bad\u7ec3  </p> <pre><code>n = int(0.9*len(data)) \ntrain_data = data[:n]\nval_data = data[n:]\n</code></pre> <p>\u5207\u5206\u8bad\u7ec3\u96c6\uff0c\u524d\u767e\u5206\u4e4b\u4e5d\u5341\u4e3a\u8bad\u7ec3\u96c6  </p> <pre><code>block_size = 8\ntrain_data[:block_size+1]\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n</code></pre> <p>\u8bad\u7ec3\u5757\u5927\u5c0f\u5b9a\u4e49\u62108\uff0c\u610f\u601d\u662f1\u63a82\uff0c1\u548c2\u63a83\uff0c\u4ee5\u6b64\u7c7b\u63a8\u3002\u8bad\u7ec3\u5757\u548c\u5f53\u524d\u5927\u6a21\u578b\u6240\u8bf4\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u662f\u4e00\u56de\u4e8b\u3002\u4e2a\u4eba\u66f4\u559c\u6b22\u7406\u89e3\u4e3a\u6ed1\u52a8\u7a97\u53e3\u8bad\u7ec3  </p> <pre><code>def get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n</code></pre> <p>\u5b9a\u4e49\u4e86\u4e00\u4e2a\u968f\u673a\u9009\u53d6\uff08batch\uff0cblock\uff09\u5927\u5c0f\u7684\u6570\u636e  </p>"},{"location":"%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/#2","title":"2.\u6a21\u578b\u6784\u5efa","text":""},{"location":"%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/#21","title":"2.1\u6700\u7b80\u5355\u7684\u4e8c\u5143\u6a21\u578b\uff0c\u635f\u5931\uff0c\u751f\u6210\u7684\u6784\u5efa\uff1a","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        #idx\u548ctarget\u5c31\u53ef\u4ee5\u7406\u89e3\u662f\u4e0a\u9762\u7684xb\u548cyb\uff0c\u4e8c\u8005\u7684\u884c\u72b6\u90fd\u662f\uff08B\uff0cT\uff09\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            logits, loss = self(idx)\n            # \u53ea\u5173\u6ce8\u6700\u540e\u4e00\u4e2a\n            logits = logits[:, -1, :] # becomes (B, C)\n            # \u901a\u8fc7softmax\n            probs = F.softmax(logits, dim=-1) # (B, C)\n\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n</code></pre> <p>\u4ece\u7b80\u5355\u7684\u4e8c\u5143\u6a21\u578b\u5f00\u59cb\u7406\u89e3\uff0cforward\u51fd\u6570\u5c31\u662f\u5c06idx\u901a\u8fc7self.token_embedding_table\u51fd\u6570\u6620\u5c04\u5230\u4e86\u8bcd\u8868\u7ef4\u5ea6\u4e0a\uff0c\u7136\u540e\u6839\u636ecross_entropy\uff08\uff09\u5bf9\u4f20\u5165\u53c2\u6570\u7684\u884c\u72b6\u8981\u6c42\u4f20\u5165\u6570\u636e\u8fd4\u56de\u635f\u5931\u51fd\u6570 generate\uff08\uff09\u51fd\u6570\u5c31\u662f\u521a\u521a\u7406\u89e3\u7684\u6ed1\u52a8\u7a97\u53e3\u5c06\u8f93\u5165\u7684idx\u4f9d\u6b21\u4ece\u8bcd\u8868\u6982\u7387\u4e2d\u901a\u8fc7torch.multinomial\u51fd\u6570\u62bd\u51fa\u4e0b\u4e00\u4e2a\u5355\u8bcd \u751f\u6210\u7ed3\u679c\u5f88\u968f\u673a\uff0c\u8fd9\u6b21\u7684\u8fd0\u884c\u7ed3\u679c\u662f\uff1aSr?qP-QWktXoL&amp;jLDJgOLVz'RIoDqHdhsV&amp;vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3 \u56e0\u4e3amaxtoken\u662f100\uff0c\u4f7f\u7528\u751f\u6210\u4e86\u4e00\u767e\u4e2a\u4e71\u4e03\u516b\u7cdf\u7684\u4e1c\u897f\u3002 \u7ed3\u679c\u5f88\u5dee\uff0c\u5f53\u7136\u6211\u4eec\u4e5f\u53ef\u4ee5\u8bad\u7ec3\u4e00\u4e0b\u6a21\u578b:</p> <pre><code>optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) #\u5b9a\u4e49\u4f18\u5316\u5668\nbatch_size = 32\nfor steps in range(100): # epoch\u662f\u4e00\u767e\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))#\u518d\u770b\u4e00\u4e0b\u7ed3\u679c\n</code></pre> <p>\u5f53\u7136\u8fd9\u53ea\u662f\u4e00\u4e2a\u5c06\u6700\u540e\u4e00\u4e2a\u5b57\u7b26\u63a8\u7406\u901a\u8fc7softmax\u51fd\u6570\u63a8\u6d4b\u4e0b\u4e00\u4e2a\uff0c\u6548\u679c\u80af\u5b9a\u4e00\u822c\u3002\u8fd0\u884c\u7ed3\u679c\u5982\u4e0b\uff1a xiKi-RJ:CgqVuUa!U?qMH.uk!sCuMXvv!CJFfx;LgRyJknOEti.?I&amp;-gPlLyulId?XlaInQ'q,lT$ 3Q&amp;sGlvHQ?mqSq-eON x?SP fUAfCAuCX:bOlgiRQWN:Mphaw tRLKuYXEaAXxrcq-gCUzeh3w!AcyaylgYWjmJM?Uzw:inaY,:C&amp;OECW:vmGGJAn3onAuMgia!ms$Vb q-gCOcPcUhOnxJGUGSPJWT:.?ujmJFoiNYWA'DxY,prZ?qdT;hoo'dHooXXlxf'WkHK&amp;u3Q?rqUi.kz;?Yx?C&amp;u3Qbfzxlyh'Vl:zyxjKXgC?eDT'QKFiBeviNxO'm!Upm$srm&amp;TqViqiBD3HBP!juEOpmZJyF$Fwfy!PlvWPFC &amp;WDdP!Ko,px x tREOE;AJ.BeXkylOVD3KHp$e?nD,.SFbWWI'ubcL!q-tU;aXmJ&amp;uGXHxJXI&amp;Z!gHRpajj;l. pTErIBjx;JKIgoCnLGXrJSP!Ac-AcbczR?    </p> <p>\u589e\u52a0\u8bad\u7ec3\u7684\u6b21\u6570\u5230\u4e00\u4e07\uff0c\u518d\u6b21\u8fd0\u884c\uff1a Iyoteng h hasbe pave pirance Rie hicomyonthar's Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey KIN d pe wither vouprrouthercc. hathe; d! My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha: h hay.JUCle n prids, r loncave w hollular s O: HIs; ht anjx?  </p> <p>DUThinqunt.</p> <p>LaZAnde. athave l. KEONH: ARThanco be y,-hedarwnoddy scace, tridesar, wnl'shenous s ls, theresseys PlorseelapinghiybHen yof GLUCEN t l-t E: I hisgothers je are!-e! QLYotouciullle'z   \u5c31\u53ef\u4ee5\u770b\u5230\u8d77\u7801\u50cf\u6837\u4e86  </p>"},{"location":"%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/%E4%BB%8E0%E6%9E%84%E5%BB%BAGPT/#22","title":"2.2\u6784\u5efa\u7279\u5f81","text":"<p>way1\uff1a</p> <pre><code>xbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n</code></pre> <p>\u663e\u800c\u6613\u89c1\u5c31\u662f\u7528\u4e4b\u524d\u91cf\u7684\u5e73\u5747\u503c\u6765\u5b9e\u73b0\u5bf9\u4e4b\u524d\u7279\u5f81\u7684\u63d0\u53d6  </p> <pre><code>wei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x # (B, T, T) @ (B, T, C) ----&gt; (B, T, C)\ntorch.allclose(xbow, xbow2)\n</code></pre> <p>\u8fd9\u662f\u901a\u8fc7\u5411\u91cf\u8ba1\u7b97\u5feb\u901f\u5f97\u5230\u7684\u5e73\u5747\u503c\u7279\u5f81</p> <p>way2(\u91cd\u5934\u620f)\uff1a</p> <pre><code>torch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)   # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x\n\nout.shape\n</code></pre> <p>\u7528\u81ea\u6ce8\u610f\u529b\u6784\u5efa</p>"},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/","title":"\u6570\u636e\u95ee\u7b54","text":""},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#pandasaistreamlit","title":"\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8epandasai\u548cstreamlit\u7684\u6570\u636e\u95ee\u7b54\u7cfb\u7edf","text":""},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#csvexcel","title":"\u7b80\u4ecb\uff1a\u7528\u6237\u53ef\u4ee5\u4f20\u5165csv\u6216excel\u8868\u683c\u6587\u4ef6\uff0c\u53ef\u4ee5\u5bf9\u6570\u636e\u8fdb\u884c\u63d0\u95ee\u3002\u754c\u9762\u5982\u4e0b:","text":"<p>\u8bbe\u8ba1\u7528\u6237\u5171\u6709\u56db\u4e2a\u6a21\u578b\u53ef\u4ee5\u9009\u62e9\uff1a * \u706b\u5c71\u5f15\u64ce * Openai\uff08\u6709\u9700\u8981\u4ee3\u7406\u95ee\u9898\uff09 * ollama(\u5728\u672c\u5730\u6709\u8fd0\u884c\u6162\u7684\u95ee\u9898) * Groq\uff08\u540c\u6837\u6709\u4ee3\u7406\u95ee\u9898\uff09</p>"},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#_1","title":"\u6838\u5fc3\u4ee3\u7801\u89e3\u8bfb:","text":""},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#1","title":"1.\u5bfc\u5165\u6838\u5fc3\u6a21\u5757","text":"<pre><code>import streamlit as st \nimport pandas as pd\nfrom pandasai.llm import OpenAI\nfrom langchain_groq.chat_models import ChatGroq\nfrom langchain_community.llms import Ollama\nfrom pandasai import SmartDataframe, clear_cache\nfrom pandasai.connectors import MySQLConnector\nfrom pandasai.responses.streamlit_response import StreamlitResponse\nfrom io import StringIO, BytesIO\nimport sys\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom huoshan import VolcanoLLM\n\nplt.rcParams['font.sans-serif'] = ['SimHei']  # \u7528\u6765\u6b63\u5e38\u663e\u793a\u4e2d\u6587\u6807\u7b7e\nplt.rcParams['axes.unicode_minus'] = False    # \u7528\u6765\u6b63\u5e38\u663e\u793a\u8d1f\u53f7\n</code></pre>"},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#2","title":"2.\u8bbe\u7f6e\u6807\u9898\uff0c\u548c\u4f20\u5165\u6570\u636e\u6a21\u5757(\u6570\u636e\u53ef\u901a\u8fc7\u4e0a\u4f20\u548c\u6570\u636e\u5e93\u7684\u65b9\u5f0f\u4f20\u5165)","text":"<pre><code>st.set_page_config(page_title=\"Talk to Your Data\")\nst.title(\"Talk to Your Data \ud83d\udc3c\")\n\n# \u4e0a\u4f20 CSV \u6216 Excel \u6587\u4ef6\nuploaded_file = st.sidebar.file_uploader(\"Upload your CSV or Excel file\", type=[\"csv\", \"xlsx\"])\ndata = None\n    if uploaded_file is not None:\n        if uploaded_file.name.endswith(\".csv\"):\n            data = pd.read_csv(uploaded_file)\n        elif uploaded_file.name.endswith(\".xlsx\"):\n            data = pd.read_excel(uploaded_file)\n\n# \u8f93\u5165 SQL \u6570\u636e\u5e93\u8fde\u63a5\u4fe1\u606f\ndb_connection = None\n\n#if st.sidebar.button(\"Setup Database Connection\"):\nwith st.sidebar.expander(\"Database Connection Settings\", expanded=True):\n     hostname = st.text_input(\"Hostname\", disabled=uploaded_file is not None)\n     port = st.text_input(\"Port\", disabled=uploaded_file is not None)\n     username = st.text_input(\"Username\", disabled=uploaded_file is not None)\n     password = st.text_input(\"Password\", type=\"password\", disabled=uploaded_file is not None)\n     db_name = st.text_input(\"Database Name\", disabled=uploaded_file is not None)\n     table = st.text_input(\"Table\", disabled=uploaded_file is not None)\n\n#if st.button(\"Connect to SQL Database\"):\n     if hostname and port and username and password and db_name and table:\n        db_connection = MySQLConnector(\n                config={\n                    \"host\": hostname,\n                    \"port\": port,\n                    \"database\": db_name,\n                    \"username\": username,\n                    \"password\": password,\n                    \"table\": table,\n                }\n            )\n\n     if not db_connection:\n        st.error(f\"Error connecting to database: {e}\")\n\n     else:\n        data = db_connection.pandas_df  # \u52a0\u8f7d\u6570\u636e\n\nif data is not None:\n    st.write(data)\n</code></pre>"},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#22pandasai","title":"2.2\u7528\u6237\u9009\u62e9pandasai\u4f20\u5165\u7684\u5927\u6a21\u578b","text":"<pre><code>    # \u9009\u62e9\u5927\u8bed\u8a00\u6a21\u578b\n    llm_choice = st.sidebar.selectbox(\"Choose a Language Model\", [\"Ollama\", \"OpenAI\", \"Groq\",\"\u706b\u5c71\"])\n\n    openai_api_key = \"\"\n    groq_api_key = \"\"\n    huoshan_api_key = \"\"\n    huoshan_endid = \"\"\n    if llm_choice == \"OpenAI\":\n        # \u8f93\u5165 OpenAI API Key\n        openai_api_key = st.sidebar.text_input(\"OpenAI API Key\", type=\"password\", disabled=not (uploaded_file or db_connection)) \n        if not openai_api_key.startswith(\"sk-\"):\n            st.warning(\"Please enter your OpenAI API key!\", icon=\"\u26a0\ufe0f\")\n    elif llm_choice == \"Groq\":\n        # \u8f93\u5165 Groq API Key\n        groq_api_key = st.sidebar.text_input(\"Groq API Key\", type=\"password\", disabled=not (uploaded_file or db_connection)) \n        if not groq_api_key.startswith(\"gsk_\"):\n            st.warning(\"Please enter your Groq API key!\", icon=\"\u26a0\ufe0f\")\n    elif llm_choice == \"\u706b\u5c71\":\n        # \u8f93\u5165 Groq API Key\n        huoshan_api_key = st.sidebar.text_input(\"\u706b\u5c71 API KEY\", type=\"password\", disabled=not (uploaded_file or db_connection))\n        huoshan_endid = st.sidebar.text_input(\"\u706b\u5c71 MODEL KEY\", type=\"password\",disabled=not (uploaded_file or db_connection))\n\n\n    def get_llm(choice, api_key=None):\n        if choice == \"OpenAI\":\n            return OpenAI(api_token=api_key)\n        elif choice == \"Groq\":\n            return ChatGroq(model_name='llama3-70b-8192', api_key=api_key)\n        elif choice == \"Ollama\":\n            return Ollama(model=\"llama3.1\")\n        elif choice == \"\u706b\u5c71\":\n            return VolcanoLLM(api_base=\"https://ark.cn-beijing.volces.com/api/v3/chat/completions\",api_key=huoshan_api_key,model=huoshan_endid)\n        else:\n            return None\n</code></pre>"},{"location":"%E6%95%B0%E6%8D%AE%E9%97%AE%E7%AD%94/%E5%B1%95%E7%A4%BA%E7%B3%BB%E7%BB%9F/#23","title":"2.3\u5bf9\u7528\u6237\u8f93\u5165\u8fdb\u884c\u54cd\u5e94","text":"<pre><code>    def generate_response(data, prompt, llm):\n        df = SmartDataframe(data, config={\n            \"llm\": llm,\n            \"custom_whitelisted_dependencies\": whitelist,\n            \"response_parser\": StreamlitResponse\n        })\n\n        # \u6355\u83b7\u6807\u51c6\u8f93\u51fa\u4ee5\u83b7\u53d6 PandasAI \u7684\u6253\u5370\u5185\u5bb9a\n        old_stdout = sys.stdout\n        sys.stdout = StringIO()\n\n        response = df.chat(prompt)\n        if 'exports/charts/temp_chart.png' in response:\n            image_path = response\n            image = Image.open(image_path)\n            st.image(image, caption='Generated Chart by PandasAI', use_column_width=True)\n        else:\n            st.write(response)\n        st.write(\"All prompts:\")\n        for i, p in enumerate(st.session_state.prompts):\n            st.write(f\"{i}: {p}\")\n</code></pre>"}]}